{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Matthew Seman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is library for scientific computing in Python. It has efficient implementation of n-dimensional array (tensor) manupulations, which is useful for machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a list into numpy array (tensor)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 4],\n",
       "       [2, 6, 9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [[1, 2, 4], [2, 6, 9]]\n",
    "a = np.array(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the dimensions of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply simple arithmetic operation on all element of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  6, 12],\n",
       "       [ 6, 18, 27]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can transpose a tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 6],\n",
       "       [4, 9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.T.shape)\n",
    "a.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply aggregate functions on the whole tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or on one dimension of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  8, 13])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 17])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do element-wise arithmetic operation on two tensors (of the same size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6, 20],\n",
       "       [ 2, 12,  9]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = np.array([[1, 2, 4], [2, 6, 9]])\n",
    "c2 = np.array([[2, 3, 5], [1, 2, 1]])\n",
    "c1 * c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to multiply all columns of a tensor by vector (for example if you want to multiply all data features by their lables) you need a trick. This multiplication shows up in calculating the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 4]\n",
      " [2 6 9]]\n",
      "[ 1 -1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 4], [2, 6, 9]])\n",
    "b = np.array([1,-1])\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to multiply the first row of a by 1 and the second row of a by -1. Simply multiplying a by b does not work because a and b do not have the same dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this multiplication we first have to assume b has one column and then repeat the column of b with the number of columns in a. We use tile function to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1],\n",
       "       [-1, -1, -1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_repeat = np.tile(b,  (a.shape[1],1)).T\n",
    "print(b_repeat.shape)\n",
    "b_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can multiply each column of a by b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  4],\n",
       "       [-2, -6, -9]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create inital random vector using numpy (using N(0,1)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0 #mean\n",
    "sigma = 1 #standard deviation\n",
    "r = np.random.normal(mu,sigma, 1000) #draws 1000 samples from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply functions on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of Normal distribution\n",
    "def normal(x, mu, sigma):\n",
    "    return np.exp( -0.5 * ((x-mu)/sigma)**2)/np.sqrt(2.0*np.pi*sigma**2)\n",
    "\n",
    "#probability of samples on the Normal distribution\n",
    "probabilities = normal(r, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has useful APIs for analysis. Here we plot the histogram of samples and also plot the probabilies to see if the samples follow the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1aa75d31100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(r,50,density=True)\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.scatter(r, probabilities, c='b', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    p = re.compile(',')\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    header = f.readline().strip()\n",
    "    varnames = p.split(header)\n",
    "    namehash = {}\n",
    "    for l in f:\n",
    "        li = p.split(l.strip())\n",
    "        xdata.append([float(x) for x in li[:-1]])\n",
    "        ydata.append(float(li[-1]))\n",
    "    \n",
    "    return np.array(xdata), np.array(ydata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming our data is x is available in numpy we use numpy to implement logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain_whole, ytrain_whole) = read_data('datasets/spambase-train.csv')\n",
    "(xtest, ytest) = read_data('datasets/spambase-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of xtrain: (3601, 54)\n",
      "The shape of ytrain: (3601,)\n",
      "The shape of xtest: (1000, 54)\n",
      "The shape of ytest: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of xtrain:\", xtrain_whole.shape)\n",
    "print(\"The shape of ytrain:\", ytrain_whole.shape)\n",
    "print(\"The shape of xtest:\", xtest.shape)\n",
    "print(\"The shape of ytest:\", ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before training make we normalize the input data (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean = np.mean(xtrain_whole, axis=0)\n",
    "xstd = np.std(xtrain_whole, axis=0)\n",
    "xtrain_normal_whole = (xtrain_whole-xmean) / xstd\n",
    "xtest_normal = (xtest-xmean) / xstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a validation set. We create an array of indecies and permute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "premute_indicies = np.random.permutation(np.arange(xtrain_whole.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the first 2600 data points as the training data and rest as the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_normal = xtrain_normal_whole[premute_indicies[:2600]]\n",
    "ytrain = ytrain_whole[premute_indicies[:2600]]\n",
    "xval_normal = xtrain_normal_whole[premute_indicies[2600:]]\n",
    "yval = ytrain_whole[premute_indicies[2600:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiallizing the weights and bias with random values from N(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(0, 1, xtrain_normal.shape[1]);\n",
    "bias = np.random.normal(0,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sigmoid function\n",
    "def sigmoid(v):\n",
    "    #return np.exp(-np.logaddexp(0, -v)) #numerically stable implementation of sigmoid function \n",
    "    return 1.0 / (1+np.exp(-v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use dot-product from numpy to calculate the margin and pass it to the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w: weight vector (numpy array of size n)\n",
    "#b: numpy array of size 1\n",
    "#returns p(y=1|x, w, b)\n",
    "def prob(x, w, b):\n",
    "    return sigmoid(np.dot(x,w) + b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate $l_2$ penalty using linalg library of numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.900268615832312"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Cross Entropy Loss} = -\\frac{1}{|D|}[\\sum_{(y^i,\\mathbf{x}^i)\\in\\mathcal{D}} \n",
    " y^i \\log p(y=1|\\mathbf{x}^i;\\mathbf{w},b)  +  (1-y^i) \\log (1 - p(y=1|\\mathbf{x}^i;\\mathbf{w},b))]+\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w: weight vector (numpy array of size n)\n",
    "#y_prob: p(y|x, w, b)\n",
    "#y_true: class variable data\n",
    "#lambda_: l2 penalty coefficient\n",
    "#returns the cross entropy loss\n",
    "def loss(w, y_prob, y_true, lambda_):\n",
    "    loss = -(1/len(y_true)) * np.sum(y_true*np.log(y_prob) + (1-y_true)*np.log(y_prob)) + (lambda_/2)*(np.linalg.norm(w) ** 2)\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x: input variables (data of size m x n with m data point and n features)\n",
    "#w: weight vector (numpy array of size n)\n",
    "#y_prob: p(y|x, w, b)\n",
    "#y_true: class variable data\n",
    "#lambda_: l2 penalty coefficient\n",
    "#returns tuple of gradient w.r.t w and w.r.t to bias\n",
    "\n",
    "def grad_w_b(x, w, y_prob, y_true, lambda_):\n",
    "    \n",
    "    grad_w = np.matmul(np.transpose(y_prob-y_true), x) + lambda_ * w\n",
    "    grad_b = np.mean(y_prob-y_true)\n",
    "    \n",
    "    return (grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lambda_ is the coeffienct of l2 norm penalty\n",
    "#learning_rate is learning rate of gradient descent algorithm\n",
    "#max_iter determines the maximum number of iterations if the gradients descent does not converge.\n",
    "#continue the training while gradient > 0.1 or the number steps is less max_iter\n",
    "\n",
    "#returns model as tuple of (weights,bias)\n",
    "\n",
    "def fit(x, y_true, learning_rate, lambda_, max_iter, verbose=0):\n",
    "    weights = np.random.normal(0, 1, x.shape[1]);\n",
    "    bias = np.random.normal(0,1,1)\n",
    "    \n",
    "    i = 1\n",
    "    loss_ = []\n",
    "    \n",
    "    #print(\"shape of w\", weights.shape)\n",
    "    #print(\"shape of x\", x.shape)\n",
    "    #print(\"shape of y\", y_true.shape)\n",
    "\n",
    "    #change the condition appropriately\n",
    "    while i < max_iter:\n",
    "        \n",
    "        #Calculate loss\n",
    "        y_prob = prob(x, weights, bias)\n",
    "        \n",
    "        loss_ = loss(weights, y_prob, y_true, lambda_)\n",
    "        \n",
    "        #Calculate Gradients\n",
    "        (grad_w, grad_b) = grad_w_b(x, weights, y_prob, y_true, lambda_)\n",
    "        \n",
    "        #Update weights and bias\n",
    "        weights = weights - learning_rate*grad_w\n",
    "        bias = bias - learning_rate*grad_b\n",
    "            \n",
    "        if verbose: #verbose is used for debugging purposes\n",
    "            #print iteration number, loss, l2 norm of gradients, l2 norm of weights\n",
    "            print(\"Iteration Number: \" + str(i) + \" / Loss: \" + str(loss_) +\n",
    "                 \" / Norm of grad_w, grad_b: \" + str(np.linalg.norm(grad_w)) + \" , \" + str(np.linalg.norm(grad_b)) +\n",
    "                 \" / Norm of weights: \" + str(np.linalg.norm(weights)))\n",
    "            pass\n",
    "        \n",
    "        #End iterations if gradient is sufficiently small\n",
    "        if np.linalg.norm(grad_w) < 0.1:\n",
    "            break\n",
    "        \n",
    "        #increment while iteration\n",
    "        i = i + 1\n",
    "        \n",
    "    return (weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y_true, model):\n",
    "    w, b = model\n",
    "    return np.sum((prob(x, w, b)>0.5).astype(np.float64) == y_true)  / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 1 / Loss: 22.977835053872354 / Norm of grad_w, grad_b: 3635.673133474757 , 0.028071710515458162 / Norm of weights: 6.298519679327563\n",
      "Iteration Number: 2 / Loss: 24.367522728623317 / Norm of grad_w, grad_b: 848.1687011496408 , 0.00243156280037814 / Norm of weights: 6.0159067616774315\n",
      "Iteration Number: 3 / Loss: 22.484189570648212 / Norm of grad_w, grad_b: 654.2275131756771 , 0.014336598725986752 / Norm of weights: 5.802598925272914\n",
      "Iteration Number: 4 / Loss: 21.146914461699257 / Norm of grad_w, grad_b: 510.8493712106777 , 0.024733334311956783 / Norm of weights: 5.633750981579898\n",
      "Iteration Number: 5 / Loss: 20.146126996522504 / Norm of grad_w, grad_b: 403.9161727056811 , 0.028210866604158318 / Norm of weights: 5.494241601630241\n",
      "Iteration Number: 6 / Loss: 19.35594145367101 / Norm of grad_w, grad_b: 335.1834905689382 , 0.026733337548448137 / Norm of weights: 5.375634374790105\n",
      "Iteration Number: 7 / Loss: 18.69521552316299 / Norm of grad_w, grad_b: 281.5675403349994 , 0.023994770709500315 / Norm of weights: 5.272332788911868\n",
      "Iteration Number: 8 / Loss: 18.12251287080377 / Norm of grad_w, grad_b: 240.1268877014385 , 0.021331525119534782 / Norm of weights: 5.180083569571775\n",
      "Iteration Number: 9 / Loss: 17.611716400986722 / Norm of grad_w, grad_b: 210.4683641486064 , 0.019007822024399677 / Norm of weights: 5.0962480347818175\n",
      "Iteration Number: 10 / Loss: 17.146879296987017 / Norm of grad_w, grad_b: 188.57953116960834 , 0.017153412554368936 / Norm of weights: 5.019250011587711\n",
      "Iteration Number: 11 / Loss: 16.7194863049063 / Norm of grad_w, grad_b: 170.93948961569717 , 0.015689667099607098 / Norm of weights: 4.948019909339624\n",
      "Iteration Number: 12 / Loss: 16.324623319851803 / Norm of grad_w, grad_b: 156.09111367554914 , 0.014450422502674153 / Norm of weights: 4.881720694335391\n",
      "Iteration Number: 13 / Loss: 15.958259801986443 / Norm of grad_w, grad_b: 143.57416866563784 , 0.01335742639479255 / Norm of weights: 4.819669809751695\n",
      "Iteration Number: 14 / Loss: 15.616717774681243 / Norm of grad_w, grad_b: 133.07767786597918 , 0.012409439833534121 / Norm of weights: 4.7613271531762775\n",
      "Iteration Number: 15 / Loss: 15.296933006691358 / Norm of grad_w, grad_b: 124.2056619297088 , 0.01161558484052962 / Norm of weights: 4.706275392351109\n",
      "Iteration Number: 16 / Loss: 14.996566334080828 / Norm of grad_w, grad_b: 116.58074240486205 , 0.010965137051493361 / Norm of weights: 4.654188329314492\n",
      "Iteration Number: 17 / Loss: 14.713827870582623 / Norm of grad_w, grad_b: 109.91115915529994 , 0.010434743126738505 / Norm of weights: 4.604803625979674\n",
      "Iteration Number: 18 / Loss: 14.447273628671443 / Norm of grad_w, grad_b: 103.98768230301013 , 0.010000197340540085 / Norm of weights: 4.557904130538347\n",
      "Iteration Number: 19 / Loss: 14.19567861424541 / Norm of grad_w, grad_b: 98.66120452489201 , 0.009641413845443552 / Norm of weights: 4.513305840456393\n",
      "Iteration Number: 20 / Loss: 13.957970572471565 / Norm of grad_w, grad_b: 93.82306363168381 , 0.009342900403726805 / Norm of weights: 4.47085004632799\n",
      "Iteration Number: 21 / Loss: 13.733195116001232 / Norm of grad_w, grad_b: 89.39166385306184 , 0.009092807632751774 / Norm of weights: 4.430397972192389\n",
      "Iteration Number: 22 / Loss: 13.520495003301068 / Norm of grad_w, grad_b: 85.30401541222352 , 0.008881917772151028 / Norm of weights: 4.391826913959293\n",
      "Iteration Number: 23 / Loss: 13.31909569393035 / Norm of grad_w, grad_b: 81.51043087610788 , 0.00870291196495591 / Norm of weights: 4.355027314512824\n",
      "Iteration Number: 24 / Loss: 13.128293946934056 / Norm of grad_w, grad_b: 77.97111797419035 , 0.008549893666872413 / Norm of weights: 4.319900466407119\n",
      "Iteration Number: 25 / Loss: 12.94744822234885 / Norm of grad_w, grad_b: 74.65388405878522 , 0.008418076802036219 / Norm of weights: 4.286356670117007\n",
      "Iteration Number: 26 / Loss: 12.775970436124009 / Norm of grad_w, grad_b: 71.53249679806952 , 0.008303566495870277 / Norm of weights: 4.25431374516063\n",
      "Iteration Number: 27 / Loss: 12.613318891081992 / Norm of grad_w, grad_b: 68.58545300717478 , 0.00820319120298609 / Norm of weights: 4.223695824264957\n",
      "Iteration Number: 28 / Loss: 12.458992267591182 / Norm of grad_w, grad_b: 65.79502624900555 , 0.008114367144479403 / Norm of weights: 4.1944323766952065\n",
      "Iteration Number: 29 / Loss: 12.312524549995347 / Norm of grad_w, grad_b: 63.14652310521857 , 0.008034987539565682 / Norm of weights: 4.166457416722494\n",
      "Iteration Number: 30 / Loss: 12.17348075290302 / Norm of grad_w, grad_b: 60.627702753073265 , 0.007963333439479284 / Norm of weights: 4.139708861531413\n",
      "Iteration Number: 31 / Loss: 12.041453316371717 / Norm of grad_w, grad_b: 58.22832354315867 , 0.0078980035882626 / Norm of weights: 4.114128010627056\n",
      "Iteration Number: 32 / Loss: 11.916059059216638 / Norm of grad_w, grad_b: 55.93978500802231 , 0.007837860149227705 / Norm of weights: 4.0896591255193275\n",
      "Iteration Number: 33 / Loss: 11.796936605404918 / Norm of grad_w, grad_b: 53.75483890190816 , 0.007781986672308455 / Norm of weights: 4.0662490935887545\n",
      "Iteration Number: 34 / Loss: 11.683744220996733 / Norm of grad_w, grad_b: 51.66734913525599 , 0.007729654766742863 / Norm of weights: 4.043847163446664\n",
      "Iteration Number: 35 / Loss: 11.576158014053028 / Norm of grad_w, grad_b: 49.672086857133145 , 0.007680296514311747 / Norm of weights: 4.022404741077897\n",
      "Iteration Number: 36 / Loss: 11.47387045719628 / Norm of grad_w, grad_b: 47.764552406494005 , 0.007633480463843729 / Norm of weights: 4.001875237076128\n",
      "Iteration Number: 37 / Loss: 11.376589194269627 / Norm of grad_w, grad_b: 45.94081984977506 , 0.007588889857427151 / Norm of weights: 3.9822139558269343\n",
      "Iteration Number: 38 / Loss: 11.28403609156419 / Norm of grad_w, grad_b: 44.19740229826407 , 0.007546302410259067 / Norm of weights: 3.9633780179237896\n",
      "Iteration Number: 39 / Loss: 11.195946492686648 / Norm of grad_w, grad_b: 42.53113740530366 , 0.007505571443948697 / Norm of weights: 3.945326307633289\n",
      "Iteration Number: 40 / Loss: 11.11206863578526 / Norm of grad_w, grad_b: 40.93909274265465 , 0.007466608460926767 / Norm of weights: 3.9280194379477975\n",
      "Iteration Number: 41 / Loss: 11.032163193198153 / Norm of grad_w, grad_b: 39.418490501084406 , 0.0074293673787448 / Norm of weights: 3.9114197266756707\n",
      "Iteration Number: 42 / Loss: 10.956002896681946 / Norm of grad_w, grad_b: 37.96665044364801 , 0.007393830660343441 / Norm of weights: 3.8954911780686645\n",
      "Iteration Number: 43 / Loss: 10.883372215900126 / Norm of grad_w, grad_b: 36.58094947431484 , 0.007359997520995973 / Norm of weights: 3.8801994655986047\n",
      "Iteration Number: 44 / Loss: 10.814067063303085 / Norm of grad_w, grad_b: 35.25879570815552 , 0.00732787429916711 / Norm of weights: 3.8655119125959048\n",
      "Iteration Number: 45 / Loss: 10.747894504386453 / Norm of grad_w, grad_b: 33.997614617552955 , 0.007297466973452293 / Norm of weights: 3.8513974684879124\n",
      "Iteration Number: 46 / Loss: 10.68467245809739 / Norm of grad_w, grad_b: 32.79484470748789 , 0.0072687757096695305 / Norm of weights: 3.8378266792799693\n",
      "Iteration Number: 47 / Loss: 10.624229377498487 / Norm of grad_w, grad_b: 31.64794023174012 , 0.007241791242630122 / Norm of weights: 3.8247716516797254\n",
      "Iteration Number: 48 / Loss: 10.56640390544139 / Norm of grad_w, grad_b: 30.554378667879515 , 0.0072164928418405995 / Norm of weights: 3.8122060108657996\n",
      "Iteration Number: 49 / Loss: 10.511044503807664 / Norm of grad_w, grad_b: 29.511670977792924 , 0.007192847580548483 / Norm of weights: 3.800104852348794\n",
      "Iteration Number: 50 / Loss: 10.458009057799368 / Norm of grad_w, grad_b: 28.517373045564366 , 0.0071708106210382995 / Norm of weights: 3.788444688679132\n",
      "Iteration Number: 51 / Loss: 10.407164458840208 / Norm of grad_w, grad_b: 27.569097063663182 , 0.007150326241763117 / Norm of weights: 3.7772033919411463\n",
      "Iteration Number: 52 / Loss: 10.358386170964966 / Norm of grad_w, grad_b: 26.664521998089796 , 0.0071313293586321574 / Norm of weights: 3.766360133058173\n",
      "Iteration Number: 53 / Loss: 10.311557786248196 / Norm of grad_w, grad_b: 25.801402580278065 , 0.007113747328320212 / Norm of weights: 3.7558953189412585\n",
      "Iteration Number: 54 / Loss: 10.266570574982698 / Norm of grad_w, grad_b: 24.977576535300205 , 0.007097501861064337 / Norm of weights: 3.7457905284652604\n",
      "Iteration Number: 55 / Loss: 10.223323036093808 / Norm of grad_w, grad_b: 24.190969958184827 , 0.007082510910190847 / Norm of weights: 3.736028448169041\n",
      "Iteration Number: 56 / Loss: 10.181720452786253 / Norm of grad_w, grad_b: 23.439600895490052 , 0.007068690442714311 / Norm of weights: 3.7265928084664353\n",
      "Iteration Number: 57 / Loss: 10.14167445776831 / Norm of grad_w, grad_b: 22.721581284511167 , 0.007055956027939465 / Norm of weights: 3.71746832103397\n",
      "Iteration Number: 58 / Loss: 10.103102611666605 / Norm of grad_w, grad_b: 22.035117456701723 , 0.007044224208137242 / Norm of weights: 3.708640617919154\n",
      "Iteration Number: 59 / Loss: 10.065927997497766 / Norm of grad_w, grad_b: 21.3785094346467 , 0.007033413636827791 / Norm of weights: 3.7000961927960803\n",
      "Iteration Number: 60 / Loss: 10.030078833345945 / Norm of grad_w, grad_b: 20.750149252251994 , 0.007023445986268441 / Norm of weights: 3.6918223446873504\n",
      "Iteration Number: 61 / Loss: 9.995488104737667 / Norm of grad_w, grad_b: 20.14851851344677 , 0.007014246637009362 / Norm of weights: 3.6838071243755235\n",
      "Iteration Number: 62 / Loss: 9.962093217625672 / Norm of grad_w, grad_b: 19.572185381824568 , 0.007005745169611317 / Norm of weights: 3.6760392836443225\n",
      "Iteration Number: 63 / Loss: 9.929835672397902 / Norm of grad_w, grad_b: 19.01980116690882 , 0.006997875682619056 / Norm of weights: 3.668508227420018\n",
      "Iteration Number: 64 / Loss: 9.898660758918178 / Norm of grad_w, grad_b: 18.490096645353848 , 0.0069905769624273055 / Norm of weights: 3.6612039688259244\n",
      "Iteration Number: 65 / Loss: 9.868517272276048 / Norm of grad_w, grad_b: 17.98187822945679 , 0.006983792530441314 / Norm of weights: 3.654117087116985\n",
      "Iteration Number: 66 / Loss: 9.83935724866846 / Norm of grad_w, grad_b: 17.494024072044724 , 0.006977470591499185 / Norm of weights: 3.6472386884255705\n",
      "Iteration Number: 67 / Loss: 9.811135720645673 / Norm of grad_w, grad_b: 17.02548017664584 , 0.006971563905348968 / Norm of weights: 3.640560369222614\n",
      "Iteration Number: 68 / Loss: 9.7838104908193 / Norm of grad_w, grad_b: 16.57525656495305 , 0.0069660296004097335 / Norm of weights: 3.6340741823787557\n",
      "Iteration Number: 69 / Loss: 9.757341923042468 / Norm of grad_w, grad_b: 16.142423539765556 , 0.0069608289463549446 / Norm of weights: 3.6277726056969226\n",
      "Iteration Number: 70 / Loss: 9.731692750021729 / Norm of grad_w, grad_b: 15.72610807052512 , 0.0069559270994136516 / Norm of weights: 3.6216485127796996\n",
      "Iteration Number: 71 / Loss: 9.706827896300638 / Norm of grad_w, grad_b: 15.325490319861169 , 0.0069512928318082624 / Norm of weights: 3.6156951460908466\n",
      "Iteration Number: 72 / Loss: 9.682714315558357 / Norm of grad_w, grad_b: 14.939800322827798 , 0.006946898254504738 / Norm of weights: 3.60990609206961\n",
      "Iteration Number: 73 / Loss: 9.659320841188336 / Norm of grad_w, grad_b: 14.568314825385807 , 0.006942718540473953 / Norm of weights: 3.6042752581582223\n",
      "Iteration Number: 74 / Loss: 9.636618049156485 / Norm of grad_w, grad_b: 14.21035428482946 , 0.0069387316539596755 / Norm of weights: 3.598796851606639\n",
      "Iteration Number: 75 / Loss: 9.61457813218224 / Norm of grad_w, grad_b: 13.865280032004549 , 0.006934918089809983 / Norm of weights: 3.5934653599235937\n",
      "Iteration Number: 76 / Loss: 9.593174784335684 / Norm of grad_w, grad_b: 13.53249159308049 , 0.0069312606257360306 / Norm of weights: 3.5882755328489924\n",
      "Iteration Number: 77 / Loss: 9.572383095197331 / Norm of grad_w, grad_b: 13.211424167148293 , 0.00692774408939039 / Norm of weights: 3.583222365729281\n",
      "Iteration Number: 78 / Loss: 9.55217945278238 / Norm of grad_w, grad_b: 12.901546254867613 , 0.006924355141378454 / Norm of weights: 3.5783010841843494\n",
      "Iteration Number: 79 / Loss: 9.53254145448648 / Norm of grad_w, grad_b: 12.602357432675197 , 0.006921082074705294 / Norm of weights: 3.5735071299616195\n",
      "Iteration Number: 80 / Loss: 9.513447825364535 / Norm of grad_w, grad_b: 12.313386266604315 , 0.006917914630690249 / Norm of weights: 3.568836147880044\n",
      "Iteration Number: 81 / Loss: 9.494878343106734 / Norm of grad_w, grad_b: 12.034188359489207 , 0.0069148438310271745 / Norm of weights: 3.5642839737736853\n",
      "Iteration Number: 82 / Loss: 9.476813769126544 / Norm of grad_w, grad_b: 11.764344525191792 , 0.006911861825412663 / Norm of weights: 3.5598466233512727\n",
      "Iteration Number: 83 / Loss: 9.459235785223266 / Norm of grad_w, grad_b: 11.503459083452599 , 0.006908961753984442 / Norm of weights: 3.5555202818945757\n",
      "Iteration Number: 84 / Loss: 9.44212693532678 / Norm of grad_w, grad_b: 11.25115826901134 , 0.006906137623695405 / Norm of weights: 3.5513012947245692\n",
      "Iteration Number: 85 / Loss: 9.425470571874325 / Norm of grad_w, grad_b: 11.007088748740719 , 0.006903384197680933 / Norm of weights: 3.547186158370144\n",
      "Iteration Number: 86 / Loss: 9.409250806408316 / Norm of grad_w, grad_b: 10.770916240679131 , 0.006900696896645732 / Norm of weights: 3.543171512379535\n",
      "Iteration Number: 87 / Loss: 9.393452464020527 / Norm of grad_w, grad_b: 10.542324229020199 , 0.006898071711295102 / Norm of weights: 3.539254131719709\n",
      "Iteration Number: 88 / Loss: 9.378061041301464 / Norm of grad_w, grad_b: 10.321012769312974 , 0.006895505124855144 / Norm of weights: 3.535430919713663\n",
      "Iteration Number: 89 / Loss: 9.36306266748463 / Norm of grad_w, grad_b: 10.106697378338175 , 0.006892994044759377 / Norm of weights: 3.5316989014699187\n",
      "Iteration Number: 90 / Loss: 9.34844406850353 / Norm of grad_w, grad_b: 9.899108003348944 , 0.006890535742626333 / Norm of weights: 3.5280552177625557\n",
      "Iteration Number: 91 / Loss: 9.334192533705288 / Norm of grad_w, grad_b: 9.69798806559514 , 0.00688812780170208 / Norm of weights: 3.5244971193237933\n",
      "Iteration Number: 92 / Loss: 9.320295884988235 / Norm of grad_w, grad_b: 9.503093573283826 , 0.006885768070999133 / Norm of weights: 3.521021961514547\n",
      "Iteration Number: 93 / Loss: 9.30674244815244 / Norm of grad_w, grad_b: 9.314192299363867 , 0.006883454625418775 / Norm of weights: 3.5176271993415003\n",
      "Iteration Number: 94 / Loss: 9.293521026271772 / Norm of grad_w, grad_b: 9.131063019757516 , 0.006881185731202035 / Norm of weights: 3.514310382792071\n",
      "Iteration Number: 95 / Loss: 9.280620874913833 / Norm of grad_w, grad_b: 8.95349480789193 , 0.006878959816109906 / Norm of weights: 3.5110691524612583\n",
      "Iteration Number: 96 / Loss: 9.268031679050395 / Norm of grad_w, grad_b: 8.781286381612247 , 0.006876775443787487 / Norm of weights: 3.50790123544672\n",
      "Iteration Number: 97 / Loss: 9.255743531515535 / Norm of grad_w, grad_b: 8.614245498777862 , 0.006874631291817877 / Norm of weights: 3.504804441490595\n",
      "Iteration Number: 98 / Loss: 9.243746912882116 / Norm of grad_w, grad_b: 8.452188398060144 , 0.006872526133019448 / Norm of weights: 3.5017766593485384\n",
      "Iteration Number: 99 / Loss: 9.232032672639157 / Norm of grad_w, grad_b: 8.294939281666824 , 0.0068704588195858535 / Norm of weights: 3.4988158533682254\n",
      "Iteration Number: 100 / Loss: 9.22059201156372 / Norm of grad_w, grad_b: 8.142329836919108 , 0.0068684282697088084 / Norm of weights: 3.495920060261195\n",
      "Iteration Number: 101 / Loss: 9.209416465190664 / Norm of grad_w, grad_b: 7.994198793799592 , 0.006866433456362403 / Norm of weights: 3.4930873860533658\n",
      "Iteration Number: 102 / Loss: 9.19849788829265 / Norm of grad_w, grad_b: 7.850391515772788 , 0.00686447339796298 / Norm of weights: 3.4903160032008973\n",
      "Iteration Number: 103 / Loss: 9.18782844029082 / Norm of grad_w, grad_b: 7.710759621355784 , 0.006862547150649954 / Norm of weights: 3.4876041478592583\n",
      "Iteration Number: 104 / Loss: 9.177400571523847 / Norm of grad_w, grad_b: 7.575160634082842 , 0.006860653801962784 / Norm of weights: 3.484950117294467\n",
      "Iteration Number: 105 / Loss: 9.167207010309689 / Norm of grad_w, grad_b: 7.443457658666599 , 0.006858792465714579 / Norm of weights: 3.482352267426437\n",
      "Iteration Number: 106 / Loss: 9.157240750740296 / Norm of grad_w, grad_b: 7.315519081308507 , 0.006856962277887526 / Norm of weights: 3.4798090104952744\n",
      "Iteration Number: 107 / Loss: 9.147495041154963 / Norm of grad_w, grad_b: 7.191218292252702 , 0.0068551623933956335 / Norm of weights: 3.477318812842158\n",
      "Iteration Number: 108 / Loss: 9.137963373242759 / Norm of grad_w, grad_b: 7.0704334288118575 , 0.00685339198358021 / Norm of weights: 3.474880192797178\n",
      "Iteration Number: 109 / Loss: 9.128639471729045 / Norm of grad_w, grad_b: 6.953047137219735 , 0.006851650234319637 / Norm of weights: 3.4724917186671487\n",
      "Iteration Number: 110 / Loss: 9.11951728460484 / Norm of grad_w, grad_b: 6.838946351783966 , 0.006849936344651482 / Norm of weights: 3.4701520068170297\n",
      "Iteration Number: 111 / Loss: 9.110590973861568 / Norm of grad_w, grad_b: 6.728022089924481 , 0.006848249525816803 / Norm of weights: 3.4678597198391103\n",
      "Iteration Number: 112 / Loss: 9.101854906696857 / Norm of grad_w, grad_b: 6.620169261787768 , 0.0068465890006502664 / Norm of weights: 3.465613564804593\n",
      "Iteration Number: 113 / Loss: 9.093303647159981 / Norm of grad_w, grad_b: 6.515286493226205 , 0.00684495400324922 / Norm of weights: 3.463412291592679\n",
      "Iteration Number: 114 / Loss: 9.084931948208283 / Norm of grad_w, grad_b: 6.413275961023356 , 0.00684334377886445 / Norm of weights: 3.4612546912926176\n",
      "Iteration Number: 115 / Loss: 9.076734744148165 / Norm of grad_w, grad_b: 6.314043239332924 , 0.006841757583964018 / Norm of weights: 3.459139594674578\n",
      "Iteration Number: 116 / Loss: 9.068707143436546 / Norm of grad_w, grad_b: 6.217497156380003 , 0.006840194686428638 / Norm of weights: 3.4570658707254927\n",
      "Iteration Number: 117 / Loss: 9.060844421820534 / Norm of grad_w, grad_b: 6.123549660548557 , 0.006838654365843232 / Norm of weights: 3.45503242524635\n",
      "Iteration Number: 118 / Loss: 9.053142015794913 / Norm of grad_w, grad_b: 6.0321156950500034 , 0.006837135913855266 / Norm of weights: 3.453038199507648\n",
      "Iteration Number: 119 / Loss: 9.045595516358619 / Norm of grad_w, grad_b: 5.9431130804331564 , 0.0068356386345753565 / Norm of weights: 3.4510821689599873\n",
      "Iteration Number: 120 / Loss: 9.038200663052823 / Norm of grad_w, grad_b: 5.856462404257553 , 0.006834161844999285 / Norm of weights: 3.4491633419969836\n",
      "Iteration Number: 121 / Loss: 9.030953338264625 / Norm of grad_w, grad_b: 5.772086917309073 , 0.006832704875435314 / Norm of weights: 3.4472807587678944\n",
      "Iteration Number: 122 / Loss: 9.023849561781503 / Norm of grad_w, grad_b: 5.689912435789873 , 0.006831267069922883 / Norm of weights: 3.445433490037521\n",
      "Iteration Number: 123 / Loss: 9.016885485582794 / Norm of grad_w, grad_b: 5.609867248964234 , 0.006829847786632067 / Norm of weights: 3.443620636091115\n",
      "Iteration Number: 124 / Loss: 9.010057388855431 / Norm of grad_w, grad_b: 5.531882031787155 , 0.006828446398235443 / Norm of weights: 3.4418413256821814\n",
      "Iteration Number: 125 / Loss: 9.003361673222171 / Norm of grad_w, grad_b: 5.455889762085729 , 0.006827062292245847 / Norm of weights: 3.4400947150211807\n",
      "Iteration Number: 126 / Loss: 8.996794858171196 / Norm of grad_w, grad_b: 5.381825641901898 , 0.006825694871315741 / Norm of weights: 3.438379986803288\n",
      "Iteration Number: 127 / Loss: 8.990353576676936 / Norm of grad_w, grad_b: 5.309627022641556 , 0.00682434355349435 / Norm of weights: 3.43669634927345\n",
      "Iteration Number: 128 / Loss: 8.98403457100246 / Norm of grad_w, grad_b: 5.239233333709043 , 0.006823007772441408 / Norm of weights: 3.4350430353271126\n",
      "Iteration Number: 129 / Loss: 8.97783468867453 / Norm of grad_w, grad_b: 5.170586014335831 , 0.006821686977595848 / Norm of weights: 3.4334193016450647\n",
      "Iteration Number: 130 / Loss: 8.971750878622931 / Norm of grad_w, grad_b: 5.103628448341589 , 0.006820380634299804 / Norm of weights: 3.431824427860949\n",
      "Iteration Number: 131 / Loss: 8.96578018747626 / Norm of grad_w, grad_b: 5.03830590159099 , 0.006819088223877998 / Norm of weights: 3.4302577157600562\n",
      "Iteration Number: 132 / Loss: 8.959919756006773 / Norm of grad_w, grad_b: 4.974565461933924 , 0.006817809243674385 / Norm of weights: 3.4287184885081103\n",
      "Iteration Number: 133 / Loss: 8.95416681571741 / Norm of grad_w, grad_b: 4.912355981438387 , 0.0068165432070468055 / Norm of weights: 3.4272060899088026\n",
      "Iteration Number: 134 / Loss: 8.948518685564498 / Norm of grad_w, grad_b: 4.851628020745104 , 0.006815289643322114 / Norm of weights: 3.425719883688907\n",
      "Iteration Number: 135 / Loss: 8.94297276880994 / Norm of grad_w, grad_b: 4.792333795391121 , 0.006814048097713711 / Norm of weights: 3.4242592528098728\n",
      "Iteration Number: 136 / Loss: 8.937526549997216 / Norm of grad_w, grad_b: 4.734427123965846 , 0.006812818131203795 / Norm of weights: 3.422823598804824\n",
      "Iteration Number: 137 / Loss: 8.932177592045623 / Norm of grad_w, grad_b: 4.677863377977973 , 0.006811599320393123 / Norm of weights: 3.421412341139976\n",
      "Iteration Number: 138 / Loss: 8.926923533457675 / Norm of grad_w, grad_b: 4.62259943332513 , 0.006810391257320192 / Norm of weights: 3.420024916599499\n",
      "Iteration Number: 139 / Loss: 8.921762085634752 / Norm of grad_w, grad_b: 4.568593623270017 , 0.006809193549253187 / Norm of weights: 3.4186607786929173\n",
      "Iteration Number: 140 / Loss: 8.916691030296327 / Norm of grad_w, grad_b: 4.515805692837739 , 0.00680800581845657 / Norm of weights: 3.417319397084175\n",
      "Iteration Number: 141 / Loss: 8.911708216998461 / Norm of grad_w, grad_b: 4.464196754558566 , 0.0068068277019353775 / Norm of weights: 3.416000257041528\n",
      "Iteration Number: 142 / Loss: 8.906811560747316 / Norm of grad_w, grad_b: 4.4137292454892805 , 0.006805658851159174 / Norm of weights: 3.4147028589074733\n",
      "Iteration Number: 143 / Loss: 8.901999039703789 / Norm of grad_w, grad_b: 4.364366885453547 , 0.006804498931768616 / Norm of weights: 3.4134267175879387\n",
      "Iteration Number: 144 / Loss: 8.897268692975445 / Norm of grad_w, grad_b: 4.316074636448828 , 0.006803347623266447 / Norm of weights: 3.412171362060014\n",
      "Iteration Number: 145 / Loss: 8.892618618492184 / Norm of grad_w, grad_b: 4.268818663173028 , 0.006802204618695391 / Norm of weights: 3.4109363348975186\n",
      "Iteration Number: 146 / Loss: 8.888046970962261 / Norm of grad_w, grad_b: 4.222566294629715 , 0.006801069624304827 / Norm of weights: 3.409721191813732\n",
      "Iteration Number: 147 / Loss: 8.883551959905306 / Norm of grad_w, grad_b: 4.177285986774922 , 0.006799942359208515 / Norm of weights: 3.4085255012206432\n",
      "Iteration Number: 148 / Loss: 8.87913184775936 / Norm of grad_w, grad_b: 4.132947286172634 , 0.0067988225550348605 / Norm of weights: 3.4073488438041\n",
      "Iteration Number: 149 / Loss: 8.87478494805883 / Norm of grad_w, grad_b: 4.0895207946296095 , 0.006797709955571616 / Norm of weights: 3.4061908121142666\n",
      "Iteration Number: 150 / Loss: 8.870509623680626 / Norm of grad_w, grad_b: 4.046978134783057 , 0.006796604316406735 / Norm of weights: 3.4050510101708156\n",
      "Iteration Number: 151 / Loss: 8.866304285155739 / Norm of grad_w, grad_b: 4.005291916616896 , 0.006795505404566519 / Norm of weights: 3.4039290530823085\n",
      "Iteration Number: 152 / Loss: 8.862167389043655 / Norm of grad_w, grad_b: 3.9644357048851337 , 0.006794412998152917 / Norm of weights: 3.4028245666792345\n",
      "Iteration Number: 153 / Loss: 8.858097436367132 / Norm of grad_w, grad_b: 3.924383987421768 , 0.006793326885980576 / Norm of weights: 3.4017371871602005\n",
      "Iteration Number: 154 / Loss: 8.85409297110498 / Norm of grad_w, grad_b: 3.8851121443188514 , 0.006792246867215525 / Norm of weights: 3.4006665607507935\n",
      "Iteration Number: 155 / Loss: 8.85015257874057 / Norm of grad_w, grad_b: 3.84659641795521 , 0.006791172751015824 / Norm of weights: 3.3996123433746335\n",
      "Iteration Number: 156 / Loss: 8.846274884863895 / Norm of grad_w, grad_b: 3.8088138838592354 , 0.006790104356175615 / Norm of weights: 3.398574200336171\n",
      "Iteration Number: 157 / Loss: 8.842458553825068 / Norm of grad_w, grad_b: 3.7717424223900538 , 0.006789041510773141 / Norm of weights: 3.3975518060148047\n",
      "Iteration Number: 158 / Loss: 8.838702287437325 / Norm of grad_w, grad_b: 3.7353606912220636 , 0.006787984051823548 / Norm of weights: 3.3965448435698833\n",
      "Iteration Number: 159 / Loss: 8.835004823727559 / Norm of grad_w, grad_b: 3.699648098618104 , 0.006786931824937128 / Norm of weights: 3.395553004656202\n",
      "Iteration Number: 160 / Loss: 8.831364935732546 / Norm of grad_w, grad_b: 3.664584777476853 , 0.0067858846839836595 / Norm of weights: 3.394575989149609\n",
      "Iteration Number: 161 / Loss: 8.827781430339165 / Norm of grad_w, grad_b: 3.63015156014054 , 0.006784842490763024 / Norm of weights: 3.3936135048823353\n",
      "Iteration Number: 162 / Loss: 8.824253147166848 / Norm of grad_w, grad_b: 3.5963299539489406 , 0.0067838051146830525 / Norm of weights: 3.392665267387704\n",
      "Iteration Number: 163 / Loss: 8.82077895749066 / Norm of grad_w, grad_b: 3.563102117525754 , 0.006782772432444474 / Norm of weights: 3.3917309996538654\n",
      "Iteration Number: 164 / Loss: 8.817357763203482 / Norm of grad_w, grad_b: 3.530450837783672 , 0.006781744327733529 / Norm of weights: 3.3908104318862287\n",
      "Iteration Number: 165 / Loss: 8.813988495815757 / Norm of grad_w, grad_b: 3.498359507634072 , 0.006780720690922363 / Norm of weights: 3.389903301278268\n",
      "Iteration Number: 166 / Loss: 8.810670115491394 / Norm of grad_w, grad_b: 3.4668121043878517 , 0.006779701418777606 / Norm of weights: 3.3890093517903943\n",
      "Iteration Number: 167 / Loss: 8.807401610118418 / Norm of grad_w, grad_b: 3.435793168833033 , 0.0067786864141769435 / Norm of weights: 3.3881283339366033\n",
      "Iteration Number: 168 / Loss: 8.804181994413108 / Norm of grad_w, grad_b: 3.4052877849754175 , 0.006777675585834099 / Norm of weights: 3.3872600045785988\n",
      "Iteration Number: 169 / Loss: 8.801010309056252 / Norm of grad_w, grad_b: 3.3752815604282826 , 0.00677666884803223 / Norm of weights: 3.3864041267271343\n",
      "Iteration Number: 170 / Loss: 8.797885619860391 / Norm of grad_w, grad_b: 3.3457606074366537 , 0.006775666120365429 / Norm of weights: 3.3855604693502928\n",
      "Iteration Number: 171 / Loss: 8.794807016966818 / Norm of grad_w, grad_b: 3.3167115245223013 , 0.006774667327488944 / Norm of weights: 3.3847288071884583\n",
      "Iteration Number: 172 / Loss: 8.791773614071237 / Norm of grad_w, grad_b: 3.2881213787350534 , 0.006773672398877593 / Norm of weights: 3.3839089205757307\n",
      "Iteration Number: 173 / Loss: 8.788784547676975 / Norm of grad_w, grad_b: 3.2599776884961176 , 0.006772681268592281 / Norm of weights: 3.3831005952675475\n",
      "Iteration Number: 174 / Loss: 8.785838976374723 / Norm of grad_w, grad_b: 3.2322684070190655 , 0.006771693875055124 / Norm of weights: 3.3823036222742857\n",
      "Iteration Number: 175 / Loss: 8.782936080147802 / Norm of grad_w, grad_b: 3.2049819062943357 , 0.006770710160832383 / Norm of weights: 3.3815177977006177\n",
      "Iteration Number: 176 / Loss: 8.780075059701968 / Norm of grad_w, grad_b: 3.178106961622652 , 0.006769730072425427 / Norm of weights: 3.380742922590428\n",
      "Iteration Number: 177 / Loss: 8.777255135818892 / Norm of grad_w, grad_b: 3.151632736683383 , 0.006768753560069638 / Norm of weights: 3.379978802777064\n",
      "Iteration Number: 178 / Loss: 8.774475548732351 / Norm of grad_w, grad_b: 3.1255487691233186 , 0.006767780577541043 / Norm of weights: 3.3792252487387406\n",
      "Iteration Number: 179 / Loss: 8.77173555752632 / Norm of grad_w, grad_b: 3.0998449566519533 , 0.006766811081970382 / Norm of weights: 3.3784820754589084\n",
      "Iteration Number: 180 / Loss: 8.769034439554149 / Norm of grad_w, grad_b: 3.0745115436289954 , 0.006765845033664664 / Norm of weights: 3.3777491022914004\n",
      "Iteration Number: 181 / Loss: 8.766371489878026 / Norm of grad_w, grad_b: 3.0495391081303493 , 0.006764882395935893 / Norm of weights: 3.377026152830183\n",
      "Iteration Number: 182 / Loss: 8.76374602072794 / Norm of grad_w, grad_b: 3.024918549478517 , 0.006763923134936813 / Norm of weights: 3.3763130547835467\n",
      "Iteration Number: 183 / Loss: 8.761157360979468 / Norm of grad_w, grad_b: 3.0006410762239217 , 0.006762967219503596 / Norm of weights: 3.3756096398525695\n",
      "Iteration Number: 184 / Loss: 8.75860485564964 / Norm of grad_w, grad_b: 2.976698194563565 , 0.00676201462100506 / Norm of weights: 3.3749157436137054\n",
      "Iteration Number: 185 / Loss: 8.756087865410247 / Norm of grad_w, grad_b: 2.953081697183571 , 0.006761065313198343 / Norm of weights: 3.374231205405337\n",
      "Iteration Number: 186 / Loss: 8.753605766117897 / Norm of grad_w, grad_b: 2.9297836525127074 , 0.006760119272091018 / Norm of weights: 3.373555868218156\n",
      "Iteration Number: 187 / Loss: 8.751157948360234 / Norm of grad_w, grad_b: 2.906796394373708 , 0.006759176475809109 / Norm of weights: 3.372889578589234\n",
      "Iteration Number: 188 / Loss: 8.74874381701773 / Norm of grad_w, grad_b: 2.884112512019749 , 0.0067582369044709625 / Norm of weights: 3.3722321864996396\n",
      "Iteration Number: 189 / Loss: 8.746362790840413 / Norm of grad_w, grad_b: 2.8617248405436975 , 0.0067573005400669815 / Norm of weights: 3.371583545275486\n",
      "Iteration Number: 190 / Loss: 8.744014302039064 / Norm of grad_w, grad_b: 2.8396264516476575 , 0.006756367366344712 / Norm of weights: 3.3709435114922766\n",
      "Iteration Number: 191 / Loss: 8.741697795890277 / Norm of grad_w, grad_b: 2.8178106447610545 , 0.006755437368699152 / Norm of weights: 3.3703119448824297\n",
      "Iteration Number: 192 / Loss: 8.739412730354903 / Norm of grad_w, grad_b: 2.7962709384952698 , 0.0067545105340684215 / Norm of weights: 3.369688708245875\n",
      "Iteration Number: 193 / Loss: 8.737158575709397 / Norm of grad_w, grad_b: 2.775001062423556 , 0.006753586850834037 / Norm of weights: 3.3690736673636\n",
      "Iteration Number: 194 / Loss: 8.734934814189572 / Norm of grad_w, grad_b: 2.753994949174781 , 0.006752666308726182 / Norm of weights: 3.3684666909140524\n",
      "Iteration Number: 195 / Loss: 8.732740939646309 / Norm of grad_w, grad_b: 2.7332467268303384 , 0.006751748898733396 / Norm of weights: 3.367867650392283\n",
      "Iteration Number: 196 / Loss: 8.730576457212795 / Norm of grad_w, grad_b: 2.712750711613162 , 0.006750834613016699 / Norm of weights: 3.3672764200317444\n",
      "Iteration Number: 197 / Loss: 8.728440882982877 / Norm of grad_w, grad_b: 2.6925014008589194 , 0.006749923444827821 / Norm of weights: 3.366692876728638\n",
      "Iteration Number: 198 / Loss: 8.726333743700085 / Norm of grad_w, grad_b: 2.672493466258696 , 0.0067490153884316745 / Norm of weights: 3.3661168999687274\n",
      "Iteration Number: 199 / Loss: 8.724254576457001 / Norm of grad_w, grad_b: 2.6527217473639175 , 0.006748110439032476 / Norm of weights: 3.3655483717565238\n",
      "Iteration Number: 200 / Loss: 8.722202928404522 / Norm of grad_w, grad_b: 2.6331812453434105 , 0.00674720859270375 / Norm of weights: 3.3649871765467654\n",
      "Iteration Number: 201 / Loss: 8.720178356470747 / Norm of grad_w, grad_b: 2.613867116983703 , 0.0067463098463218015 / Norm of weights: 3.364433201178102\n",
      "Iteration Number: 202 / Loss: 8.718180427089045 / Norm of grad_w, grad_b: 2.5947746689232947 , 0.006745414197502683 / Norm of weights: 3.3638863348089125\n",
      "Iteration Number: 203 / Loss: 8.716208715935059 / Norm of grad_w, grad_b: 2.575899352112276 , 0.006744521644542325 / Norm of weights: 3.3633464688551764\n",
      "Iteration Number: 204 / Loss: 8.714262807672256 / Norm of grad_w, grad_b: 2.5572367564887357 , 0.006743632186359822 / Norm of weights: 3.3628134969303285\n",
      "Iteration Number: 205 / Loss: 8.712342295705774 / Norm of grad_w, grad_b: 2.5387826058639087 , 0.006742745822443912 / Norm of weights: 3.3622873147870247\n",
      "Iteration Number: 206 / Loss: 8.710446781944206 / Norm of grad_w, grad_b: 2.520532753007839 , 0.006741862552801916 / Norm of weights: 3.361767820260756\n",
      "Iteration Number: 207 / Loss: 8.70857587656912 / Norm of grad_w, grad_b: 2.5024831749281486 , 0.006740982377911727 / Norm of weights: 3.3612549132152414\n",
      "Iteration Number: 208 / Loss: 8.706729197811933 / Norm of grad_w, grad_b: 2.4846299683343553 , 0.006740105298676355 / Norm of weights: 3.3607484954895344\n",
      "Iteration Number: 209 / Loss: 8.704906371737984 / Norm of grad_w, grad_b: 2.4669693452805013 , 0.006739231316380872 / Norm of weights: 3.360248470846798\n",
      "Iteration Number: 210 / Loss: 8.703107032037476 / Norm of grad_w, grad_b: 2.4494976289792274 , 0.006738360432651775 / Norm of weights: 3.359754744924669\n",
      "Iteration Number: 211 / Loss: 8.701330819823076 / Norm of grad_w, grad_b: 2.4322112497806705 , 0.006737492649418652 / Norm of weights: 3.3592672251871747\n",
      "Iteration Number: 212 / Loss: 8.699577383433924 / Norm of grad_w, grad_b: 2.4151067413094394 , 0.006736627968877993 / Norm of weights: 3.3587858208781336\n",
      "Iteration Number: 213 / Loss: 8.697846378245847 / Norm of grad_w, grad_b: 2.3981807367538823 , 0.006735766393458988 / Norm of weights: 3.3583104429760025\n",
      "Iteration Number: 214 / Loss: 8.696137466487533 / Norm of grad_w, grad_b: 2.3814299653010846 , 0.0067349079257914835 / Norm of weights: 3.357841004150108\n",
      "Iteration Number: 215 / Loss: 8.694450317062463 / Norm of grad_w, grad_b: 2.364851248712324 , 0.006734052568675532 / Norm of weights: 3.3573774187182206\n",
      "Iteration Number: 216 / Loss: 8.692784605376417 / Norm of grad_w, grad_b: 2.3484414980330475 , 0.006733200325053033 / Norm of weights: 3.3569196026054255\n",
      "Iteration Number: 217 / Loss: 8.691140013170337 / Norm of grad_w, grad_b: 2.3321977104321343 , 0.00673235119798073 / Norm of weights: 3.356467473304245\n",
      "Iteration Number: 218 / Loss: 8.689516228358379 / Norm of grad_w, grad_b: 2.316116966165278 , 0.006731505190605188 / Norm of weights: 3.356020949835964\n",
      "Iteration Number: 219 / Loss: 8.687912944870945 / Norm of grad_w, grad_b: 2.3001964256573917 , 0.006730662306138989 / Norm of weights: 3.355579952713132\n",
      "Iteration Number: 220 / Loss: 8.686329862502577 / Norm of grad_w, grad_b: 2.284433326699467 , 0.00672982254783868 / Norm of weights: 3.3551444039031804\n",
      "Iteration Number: 221 / Loss: 8.684766686764469 / Norm of grad_w, grad_b: 2.2688249817549817 , 0.00672898591898372 / Norm of weights: 3.3547142267931394\n",
      "Iteration Number: 222 / Loss: 8.683223128741513 / Norm of grad_w, grad_b: 2.253368775371682 , 0.0067281524228571775 / Norm of weights: 3.3542893461554018\n",
      "Iteration Number: 223 / Loss: 8.68169890495367 / Norm of grad_w, grad_b: 2.2380621616943204 , 0.006727322062727474 / Norm of weights: 3.353869688114505\n",
      "Iteration Number: 224 / Loss: 8.680193737221552 / Norm of grad_w, grad_b: 2.2229026620743673 , 0.0067264948418311885 / Norm of weights: 3.3534551801148913\n",
      "Iteration Number: 225 / Loss: 8.678707352536037 / Norm of grad_w, grad_b: 2.207887862772607 , 0.006725670763357257 / Norm of weights: 3.353045750889626\n",
      "Iteration Number: 226 / Loss: 8.677239482931821 / Norm of grad_w, grad_b: 2.193015412751043 , 0.0067248498304321395 / Norm of weights: 3.352641330430025\n",
      "Iteration Number: 227 / Loss: 8.675789865364736 / Norm of grad_w, grad_b: 2.1782830215502553 , 0.006724032046105877 / Norm of weights: 3.3522418499561724\n",
      "Iteration Number: 228 / Loss: 8.674358241592731 / Norm of grad_w, grad_b: 2.163688457248971 , 0.006723217413339354 / Norm of weights: 3.351847241888298\n",
      "Iteration Number: 229 / Loss: 8.672944358060375 / Norm of grad_w, grad_b: 2.1492295445022553 , 0.006722405934992272 / Norm of weights: 3.35145743981898\n",
      "Iteration Number: 230 / Loss: 8.671547965786784 / Norm of grad_w, grad_b: 2.1349041626553227 , 0.006721597613812052 / Norm of weights: 3.3510723784861494\n",
      "Iteration Number: 231 / Loss: 8.670168820256828 / Norm of grad_w, grad_b: 2.1207102439297456 , 0.006720792452423709 / Norm of weights: 3.350691993746874\n",
      "Iteration Number: 232 / Loss: 8.668806681315544 / Norm of grad_w, grad_b: 2.106645771679141 , 0.006719990453320287 / Norm of weights: 3.350316222551886\n",
      "Iteration Number: 233 / Loss: 8.667461313065608 / Norm of grad_w, grad_b: 2.0927087787115566 , 0.006719191618854104 / Norm of weights: 3.3499450029208395\n",
      "Iteration Number: 234 / Loss: 8.666132483767807 / Norm of grad_w, grad_b: 2.078897345675766 , 0.006718395951228748 / Norm of weights: 3.3495782739182647\n",
      "Iteration Number: 235 / Loss: 8.664819965744373 / Norm of grad_w, grad_b: 2.0652095995088593 , 0.006717603452491858 / Norm of weights: 3.349215975630206\n",
      "Iteration Number: 236 / Loss: 8.663523535285108 / Norm of grad_w, grad_b: 2.051643711942746 , 0.00671681412452811 / Norm of weights: 3.348858049141511\n",
      "Iteration Number: 237 / Loss: 8.662242972556204 / Norm of grad_w, grad_b: 2.038197898066958 , 0.006716027969053271 / Norm of weights: 3.348504436513759\n",
      "Iteration Number: 238 / Loss: 8.660978061511655 / Norm of grad_w, grad_b: 2.024870414945659 , 0.00671524498760856 / Norm of weights: 3.3481550807638003\n",
      "Iteration Number: 239 / Loss: 8.659728589807198 / Norm of grad_w, grad_b: 2.0116595602865757 , 0.006714465181555656 / Norm of weights: 3.3478099258428937\n",
      "Iteration Number: 240 / Loss: 8.658494348716694 / Norm of grad_w, grad_b: 1.9985636711596566 , 0.006713688552072029 / Norm of weights: 3.3474689166164175\n",
      "Iteration Number: 241 / Loss: 8.657275133050847 / Norm of grad_w, grad_b: 1.9855811227636166 , 0.006712915100146982 / Norm of weights: 3.3471319988441373\n",
      "Iteration Number: 242 / Loss: 8.656070741078222 / Norm of grad_w, grad_b: 1.9727103272382307 , 0.006712144826577981 / Norm of weights: 3.346799119161016\n",
      "Iteration Number: 243 / Loss: 8.65488097444846 / Norm of grad_w, grad_b: 1.9599497325206665 , 0.006711377731967411 / Norm of weights: 3.3464702250585447\n",
      "Iteration Number: 244 / Loss: 8.653705638117634 / Norm of grad_w, grad_b: 1.947297821243968 , 0.0067106138167199155 / Norm of weights: 3.346145264866577\n",
      "Iteration Number: 245 / Loss: 8.652544540275658 / Norm of grad_w, grad_b: 1.9347531096760635 , 0.006709853081039823 / Norm of weights: 3.3458241877356567\n",
      "Iteration Number: 246 / Loss: 8.651397492275718 / Norm of grad_w, grad_b: 1.9223141466975961 , 0.006709095524929102 / Norm of weights: 3.3455069436198186\n",
      "Iteration Number: 247 / Loss: 8.650264308565626 / Norm of grad_w, grad_b: 1.9099795128170587 , 0.0067083411481857385 / Norm of weights: 3.345193483259848\n",
      "Iteration Number: 248 / Loss: 8.649144806621047 / Norm of grad_w, grad_b: 1.8977478192216974 , 0.006707589950402179 / Norm of weights: 3.3448837581669855\n",
      "Iteration Number: 249 / Loss: 8.648038806880539 / Norm of grad_w, grad_b: 1.8856177068627837 , 0.006706841930964295 / Norm of weights: 3.3445777206070635\n",
      "Iteration Number: 250 / Loss: 8.646946132682372 / Norm of grad_w, grad_b: 1.8735878455737702 , 0.006706097089050586 / Norm of weights: 3.344275323585063\n",
      "Iteration Number: 251 / Loss: 8.645866610203033 / Norm of grad_w, grad_b: 1.8616569332202344 , 0.006705355423631446 / Norm of weights: 3.343976520830068\n",
      "Iteration Number: 252 / Loss: 8.64480006839737 / Norm of grad_w, grad_b: 1.8498236948800246 , 0.0067046169334689905 / Norm of weights: 3.3436812667806204\n",
      "Iteration Number: 253 / Loss: 8.643746338940353 / Norm of grad_w, grad_b: 1.8380868820527212 , 0.006703881617116923 / Norm of weights: 3.3433895165704506\n",
      "Iteration Number: 254 / Loss: 8.642705256170377 / Norm of grad_w, grad_b: 1.8264452718970243 , 0.006703149472920518 / Norm of weights: 3.343101226014578\n",
      "Iteration Number: 255 / Loss: 8.641676657034058 / Norm of grad_w, grad_b: 1.8148976664950245 , 0.006702420499017264 / Norm of weights: 3.3428163515957703\n",
      "Iteration Number: 256 / Loss: 8.640660381032495 / Norm of grad_w, grad_b: 1.803442892142346 , 0.006701694693337022 / Norm of weights: 3.3425348504513446\n",
      "Iteration Number: 257 / Loss: 8.639656270168924 / Norm of grad_w, grad_b: 1.7920797986631098 , 0.006700972053602966 / Norm of weights: 3.3422566803603098\n",
      "Iteration Number: 258 / Loss: 8.638664168897742 / Norm of grad_w, grad_b: 1.7808072587485972 , 0.0067002525773324105 / Norm of weights: 3.3419817997308354\n",
      "Iteration Number: 259 / Loss: 8.637683924074876 / Norm of grad_w, grad_b: 1.7696241673189845 , 0.006699536261837785 / Norm of weights: 3.3417101675880265\n",
      "Iteration Number: 260 / Loss: 8.636715384909383 / Norm of grad_w, grad_b: 1.7585294409069208 , 0.006698823104227816 / Norm of weights: 3.341441743562021\n",
      "Iteration Number: 261 / Loss: 8.635758402916359 / Norm of grad_w, grad_b: 1.7475220170623407 , 0.006698113101408877 / Norm of weights: 3.341176487876369\n",
      "Iteration Number: 262 / Loss: 8.634812831870999 / Norm of grad_w, grad_b: 1.736600853777495 , 0.006697406250086455 / Norm of weights: 3.3409143613367114\n",
      "Iteration Number: 263 / Loss: 8.633878527763857 / Norm of grad_w, grad_b: 1.7257649289316006 , 0.006696702546766622 / Norm of weights: 3.340655325319732\n",
      "Iteration Number: 264 / Loss: 8.632955348757228 / Norm of grad_w, grad_b: 1.7150132397541569 , 0.006696001987757807 / Norm of weights: 3.3403993417623856\n",
      "Iteration Number: 265 / Loss: 8.632043155142636 / Norm of grad_w, grad_b: 1.7043448023063663 , 0.006695304569172459 / Norm of weights: 3.3401463731513865\n",
      "Iteration Number: 266 / Loss: 8.631141809299379 / Norm of grad_w, grad_b: 1.6937586509798443 , 0.006694610286929073 / Norm of weights: 3.3398963825129555\n",
      "Iteration Number: 267 / Loss: 8.630251175654127 / Norm of grad_w, grad_b: 1.6832538380120958 , 0.006693919136754018 / Norm of weights: 3.339649333402811\n",
      "Iteration Number: 268 / Loss: 8.629371120641498 / Norm of grad_w, grad_b: 1.6728294330178735 , 0.00669323111418365 / Norm of weights: 3.339405189896408\n",
      "Iteration Number: 269 / Loss: 8.628501512665645 / Norm of grad_w, grad_b: 1.6624845225361544 , 0.0066925462145663965 / Norm of weights: 3.339163916579406\n",
      "Iteration Number: 270 / Loss: 8.62764222206275 / Norm of grad_w, grad_b: 1.6522182095918 , 0.006691864433065 / Norm of weights: 3.3389254785383637\n",
      "Iteration Number: 271 / Loss: 8.626793121064463 / Norm of grad_w, grad_b: 1.6420296132715695 , 0.006691185764658701 / Norm of weights: 3.3386898413516604\n",
      "Iteration Number: 272 / Loss: 8.625954083762222 / Norm of grad_w, grad_b: 1.6319178683138653 , 0.006690510204145621 / Norm of weights: 3.338456971080625\n",
      "Iteration Number: 273 / Loss: 8.625124986072446 / Norm of grad_w, grad_b: 1.621882124711686 , 0.006689837746145064 / Norm of weights: 3.338226834260877\n",
      "Iteration Number: 274 / Loss: 8.62430570570255 / Norm of grad_w, grad_b: 1.6119215473283262 , 0.006689168385099963 / Norm of weights: 3.33799939789387\n",
      "Iteration Number: 275 / Loss: 8.6234961221178 / Norm of grad_w, grad_b: 1.6020353155252767 , 0.0066885021152793805 / Norm of weights: 3.33777462943863\n",
      "Iteration Number: 276 / Loss: 8.622696116508942 / Norm of grad_w, grad_b: 1.5922226228020162 , 0.006687838930780908 / Norm of weights: 3.3375524968036867\n",
      "Iteration Number: 277 / Loss: 8.621905571760607 / Norm of grad_w, grad_b: 1.5824826764470676 , 0.006687178825533347 / Norm of weights: 3.337332968339189\n",
      "Iteration Number: 278 / Loss: 8.621124372420468 / Norm of grad_w, grad_b: 1.5728146972000574 , 0.006686521793299104 / Norm of weights: 3.337116012829201\n",
      "Iteration Number: 279 / Loss: 8.620352404669111 / Norm of grad_w, grad_b: 1.5632179189242676 , 0.006685867827676904 / Norm of weights: 3.336901599484175\n",
      "Iteration Number: 280 / Loss: 8.619589556290627 / Norm of grad_w, grad_b: 1.5536915882893942 , 0.0066852169221043875 / Norm of weights: 3.336689697933593\n",
      "Iteration Number: 281 / Loss: 8.618835716643874 / Norm of grad_w, grad_b: 1.5442349644640416 , 0.0066845690698607504 / Norm of weights: 3.3364802782187755\n",
      "Iteration Number: 282 / Loss: 8.618090776634414 / Norm of grad_w, grad_b: 1.5348473188176237 , 0.006683924264069332 / Norm of weights: 3.336273310785852\n",
      "Iteration Number: 283 / Loss: 8.617354628687108 / Norm of grad_w, grad_b: 1.5255279346314694 , 0.006683282497700437 / Norm of weights: 3.336068766478882\n",
      "Iteration Number: 284 / Loss: 8.6166271667193 / Norm of grad_w, grad_b: 1.5162761068185382 , 0.006682643763573896 / Norm of weights: 3.3358666165331416\n",
      "Iteration Number: 285 / Loss: 8.615908286114676 / Norm of grad_w, grad_b: 1.5070911416516857 , 0.00668200805436177 / Norm of weights: 3.3356668325685437\n",
      "Iteration Number: 286 / Loss: 8.61519788369765 / Norm of grad_w, grad_b: 1.4979723565000331 , 0.006681375362591162 / Norm of weights: 3.3354693865832115\n",
      "Iteration Number: 287 / Loss: 8.614495857708391 / Norm of grad_w, grad_b: 1.4889190795731795 , 0.006680745680646842 / Norm of weights: 3.335274250947191\n",
      "Iteration Number: 288 / Loss: 8.613802107778351 / Norm of grad_w, grad_b: 1.479930649673029 , 0.006680119000773871 / Norm of weights: 3.335081398396298\n",
      "Iteration Number: 289 / Loss: 8.613116534906402 / Norm of grad_w, grad_b: 1.4710064159529106 , 0.0066794953150805915 / Norm of weights: 3.334890802026098\n",
      "Iteration Number: 290 / Loss: 8.612439041435447 / Norm of grad_w, grad_b: 1.4621457376837306 , 0.006678874615541014 / Norm of weights: 3.334702435286018\n",
      "Iteration Number: 291 / Loss: 8.611769531029596 / Norm of grad_w, grad_b: 1.453347984026962 , 0.006678256893997753 / Norm of weights: 3.3345162719735804\n",
      "Iteration Number: 292 / Loss: 8.611107908651809 / Norm of grad_w, grad_b: 1.4446125338142088 , 0.0066776421421646396 / Norm of weights: 3.3343322862287654\n",
      "Iteration Number: 293 / Loss: 8.610454080542056 / Norm of grad_w, grad_b: 1.4359387753330772 , 0.00667703035162942 / Norm of weights: 3.334150452528484\n",
      "Iteration Number: 294 / Loss: 8.609807954195935 / Norm of grad_w, grad_b: 1.427326106119193 , 0.006676421513856472 / Norm of weights: 3.333970745681176\n",
      "Iteration Number: 295 / Loss: 8.609169438343764 / Norm of grad_w, grad_b: 1.4187739327541786 , 0.006675815620189498 / Norm of weights: 3.333793140821514\n",
      "Iteration Number: 296 / Loss: 8.608538442930124 / Norm of grad_w, grad_b: 1.4102816706692543 , 0.0066752126618541425 / Norm of weights: 3.3336176134052224\n",
      "Iteration Number: 297 / Loss: 8.607914879093826 / Norm of grad_w, grad_b: 1.4018487439544374 , 0.006674612629960754 / Norm of weights: 3.3334441392040013\n",
      "Iteration Number: 298 / Loss: 8.607298659148334 / Norm of grad_w, grad_b: 1.3934745851730477 , 0.006674015515506851 / Norm of weights: 3.3332726943005544\n",
      "Iteration Number: 299 / Loss: 8.606689696562578 / Norm of grad_w, grad_b: 1.3851586351813217 , 0.006673421309379912 / Norm of weights: 3.333103255083722\n",
      "Iteration Number: 300 / Loss: 8.606087905942186 / Norm of grad_w, grad_b: 1.3769003429531284 , 0.006672830002360036 / Norm of weights: 3.332935798243708\n",
      "Iteration Number: 301 / Loss: 8.605493203011111 / Norm of grad_w, grad_b: 1.3686991654093834 , 0.006672241585122273 / Norm of weights: 3.332770300767408\n",
      "Iteration Number: 302 / Loss: 8.604905504593637 / Norm of grad_w, grad_b: 1.3605545672521209 , 0.006671656048239498 / Norm of weights: 3.33260673993383\n",
      "Iteration Number: 303 / Loss: 8.604324728596758 / Norm of grad_w, grad_b: 1.3524660208032047 , 0.006671073382184858 / Norm of weights: 3.3324450933096057\n",
      "Iteration Number: 304 / Loss: 8.603750793992932 / Norm of grad_w, grad_b: 1.3444330058472245 , 0.006670493577334284 / Norm of weights: 3.3322853387445925\n",
      "Iteration Number: 305 / Loss: 8.603183620803177 / Norm of grad_w, grad_b: 1.3364550094786989 , 0.006669916623969075 / Norm of weights: 3.3321274543675643\n",
      "Iteration Number: 306 / Loss: 8.602623130080536 / Norm of grad_w, grad_b: 1.328531525953361 , 0.00666934251227834 / Norm of weights: 3.331971418581983\n",
      "Iteration Number: 307 / Loss: 8.602069243893846 / Norm of grad_w, grad_b: 1.3206620565433425 , 0.006668771232361587 / Norm of weights: 3.3318172100618573\n",
      "Iteration Number: 308 / Loss: 8.601521885311861 / Norm of grad_w, grad_b: 1.3128461093961843 , 0.006668202774231092 / Norm of weights: 3.3316648077476776\n",
      "Iteration Number: 309 / Loss: 8.600980978387689 / Norm of grad_w, grad_b: 1.305083199397599 , 0.006667637127814325 / Norm of weights: 3.3315141908424377\n",
      "Iteration Number: 310 / Loss: 8.600446448143543 / Norm of grad_w, grad_b: 1.29737284803775 , 0.006667074282956526 / Norm of weights: 3.331365338807723\n",
      "Iteration Number: 311 / Loss: 8.599918220555795 / Norm of grad_w, grad_b: 1.2897145832809607 , 0.006666514229422887 / Norm of weights: 3.331218231359882\n",
      "Iteration Number: 312 / Loss: 8.599396222540319 / Norm of grad_w, grad_b: 1.282107939438939 , 0.006665956956901039 / Norm of weights: 3.331072848466268\n",
      "Iteration Number: 313 / Loss: 8.598880381938141 / Norm of grad_w, grad_b: 1.274552457047044 , 0.006665402455003449 / Norm of weights: 3.3309291703415504\n",
      "Iteration Number: 314 / Loss: 8.598370627501357 / Norm of grad_w, grad_b: 1.2670476827438797 , 0.006664850713269566 / Norm of weights: 3.3307871774440985\n",
      "Iteration Number: 315 / Loss: 8.597866888879324 / Norm of grad_w, grad_b: 1.2595931691538125 , 0.006664301721168356 / Norm of weights: 3.330646850472432\n",
      "Iteration Number: 316 / Loss: 8.597369096605142 / Norm of grad_w, grad_b: 1.252188474772524 , 0.006663755468100384 / Norm of weights: 3.33050817036174\n",
      "Iteration Number: 317 / Loss: 8.596877182082379 / Norm of grad_w, grad_b: 1.2448331638553916 , 0.006663211943400109 / Norm of weights: 3.3303711182804587\n",
      "Iteration Number: 318 / Loss: 8.596391077572036 / Norm of grad_w, grad_b: 1.2375268063086782 , 0.006662671136338213 / Norm of weights: 3.330235675626921\n",
      "Iteration Number: 319 / Loss: 8.595910716179805 / Norm of grad_w, grad_b: 1.230268977583376 , 0.006662133036123608 / Norm of weights: 3.3301018240260616\n",
      "Iteration Number: 320 / Loss: 8.595436031843542 / Norm of grad_w, grad_b: 1.2230592585716917 , 0.006661597631905796 / Norm of weights: 3.329969545326182\n",
      "Iteration Number: 321 / Loss: 8.594966959320969 / Norm of grad_w, grad_b: 1.2158972355060886 , 0.006661064912776863 / Norm of weights: 3.3298388215957813\n",
      "Iteration Number: 322 / Loss: 8.594503434177632 / Norm of grad_w, grad_b: 1.2087824998606862 , 0.006660534867773732 / Norm of weights: 3.3297096351204347\n",
      "Iteration Number: 323 / Loss: 8.59404539277507 / Norm of grad_w, grad_b: 1.2017146482551915 , 0.006660007485880134 / Norm of weights: 3.3295819683997383\n",
      "Iteration Number: 324 / Loss: 8.593592772259218 / Norm of grad_w, grad_b: 1.1946932823610652 , 0.0066594827560287435 / Norm of weights: 3.329455804144298\n",
      "Iteration Number: 325 / Loss: 8.593145510548993 / Norm of grad_w, grad_b: 1.187718008809865 , 0.006658960667103225 / Norm of weights: 3.3293311252727844\n",
      "Iteration Number: 326 / Loss: 8.592703546325147 / Norm of grad_w, grad_b: 1.1807884391039376 , 0.0066584412079402 / Norm of weights: 3.3292079149090275\n",
      "Iteration Number: 327 / Loss: 8.592266819019267 / Norm of grad_w, grad_b: 1.1739041895290632 , 0.006657924367331241 / Norm of weights: 3.329086156379171\n",
      "Iteration Number: 328 / Loss: 8.591835268803013 / Norm of grad_w, grad_b: 1.1670648810692708 , 0.006657410134024924 / Norm of weights: 3.3289658332088763\n",
      "Iteration Number: 329 / Loss: 8.591408836577546 / Norm of grad_w, grad_b: 1.1602701393235786 , 0.006656898496728577 / Norm of weights: 3.3288469291205685\n",
      "Iteration Number: 330 / Loss: 8.590987463963128 / Norm of grad_w, grad_b: 1.153519594424708 , 0.006656389444110372 / Norm of weights: 3.328729428030739\n",
      "Iteration Number: 331 / Loss: 8.590571093288935 / Norm of grad_w, grad_b: 1.1468128809597018 , 0.006655882964801089 / Norm of weights: 3.328613314047289\n",
      "Iteration Number: 332 / Loss: 8.590159667583046 / Norm of grad_w, grad_b: 1.1401496378922655 , 0.006655379047396057 / Norm of weights: 3.328498571466921\n",
      "Iteration Number: 333 / Loss: 8.589753130562599 / Norm of grad_w, grad_b: 1.1335295084870634 , 0.0066548776804568385 / Norm of weights: 3.3283851847725745\n",
      "Iteration Number: 334 / Loss: 8.589351426624127 / Norm of grad_w, grad_b: 1.1269521402355545 , 0.0066543788525132075 / Norm of weights: 3.328273138630905\n",
      "Iteration Number: 335 / Loss: 8.588954500834078 / Norm of grad_w, grad_b: 1.120417184783686 , 0.006653882552064837 / Norm of weights: 3.328162417889806\n",
      "Iteration Number: 336 / Loss: 8.588562298919486 / Norm of grad_w, grad_b: 1.1139242978610462 , 0.006653388767582985 / Norm of weights: 3.328053007575974\n",
      "Iteration Number: 337 / Loss: 8.588174767258806 / Norm of grad_w, grad_b: 1.1074731392117476 , 0.006652897487512329 / Norm of weights: 3.3279448928925124\n",
      "Iteration Number: 338 / Loss: 8.587791852872927 / Norm of grad_w, grad_b: 1.1010633725267711 , 0.006652408700272574 / Norm of weights: 3.327838059216576\n",
      "Iteration Number: 339 / Loss: 8.587413503416307 / Norm of grad_w, grad_b: 1.094694665377857 , 0.006651922394260201 / Norm of weights: 3.3277324920970557\n",
      "Iteration Number: 340 / Loss: 8.587039667168302 / Norm of grad_w, grad_b: 1.0883666891528279 , 0.006651438557850045 / Norm of weights: 3.3276281772523006\n",
      "Iteration Number: 341 / Loss: 8.586670293024605 / Norm of grad_w, grad_b: 1.0820791189923298 , 0.006650957179396884 / Norm of weights: 3.3275251005678776\n",
      "Iteration Number: 342 / Loss: 8.586305330488862 / Norm of grad_w, grad_b: 1.0758316337280054 , 0.006650478247237175 / Norm of weights: 3.3274232480943686\n",
      "Iteration Number: 343 / Loss: 8.585944729664412 / Norm of grad_w, grad_b: 1.0696239158219334 , 0.006650001749690479 / Norm of weights: 3.3273226060452017\n",
      "Iteration Number: 344 / Loss: 8.585588441246168 / Norm of grad_w, grad_b: 1.0634556513074613 , 0.006649527675060972 / Norm of weights: 3.32722316079452\n",
      "Iteration Number: 345 / Loss: 8.585236416512656 / Norm of grad_w, grad_b: 1.057326529731225 , 0.00664905601163918 / Norm of weights: 3.3271248988750806\n",
      "Iteration Number: 346 / Loss: 8.584888607318135 / Norm of grad_w, grad_b: 1.0512362440965188 , 0.006648586747703239 / Norm of weights: 3.3270278069761963\n",
      "Iteration Number: 347 / Loss: 8.58454496608493 / Norm of grad_w, grad_b: 1.0451844908077517 , 0.006648119871520418 / Norm of weights: 3.326931871941699\n",
      "Iteration Number: 348 / Loss: 8.584205445795813 / Norm of grad_w, grad_b: 1.039170969616184 , 0.0066476553713486625 / Norm of weights: 3.3268370807679424\n",
      "Iteration Number: 349 / Loss: 8.583869999986558 / Norm of grad_w, grad_b: 1.0331953835667436 , 0.006647193235437866 / Norm of weights: 3.3267434206018347\n",
      "Iteration Number: 350 / Loss: 8.583538582738587 / Norm of grad_w, grad_b: 1.0272574389459956 , 0.006646733452031386 / Norm of weights: 3.3266508787389037\n",
      "Iteration Number: 351 / Loss: 8.583211148671776 / Norm of grad_w, grad_b: 1.0213568452311634 , 0.0066462760093673485 / Norm of weights: 3.326559442621388\n",
      "Iteration Number: 352 / Loss: 8.582887652937332 / Norm of grad_w, grad_b: 1.0154933150402727 , 0.00664582089567993 / Norm of weights: 3.326469099836365\n",
      "Iteration Number: 353 / Loss: 8.582568051210828 / Norm of grad_w, grad_b: 1.0096665640832403 , 0.006645368099200873 / Norm of weights: 3.3263798381139003\n",
      "Iteration Number: 354 / Loss: 8.582252299685317 / Norm of grad_w, grad_b: 1.0038763111140256 , 0.006644917608160542 / Norm of weights: 3.326291645325232\n",
      "Iteration Number: 355 / Loss: 8.581940355064578 / Norm of grad_w, grad_b: 0.9981222778838073 , 0.006644469410789335 / Norm of weights: 3.326204509480979\n",
      "Iteration Number: 356 / Loss: 8.58163217455646 / Norm of grad_w, grad_b: 0.9924041890949785 , 0.00664402349531894 / Norm of weights: 3.3261184187293784\n",
      "Iteration Number: 357 / Loss: 8.58132771586633 / Norm of grad_w, grad_b: 0.9867217723562108 , 0.00664357984998354 / Norm of weights: 3.3260333613545523\n",
      "Iteration Number: 358 / Loss: 8.581026937190641 / Norm of grad_w, grad_b: 0.9810747581383589 , 0.006643138463020968 / Norm of weights: 3.325949325774793\n",
      "Iteration Number: 359 / Loss: 8.580729797210571 / Norm of grad_w, grad_b: 0.9754628797312738 , 0.006642699322673891 / Norm of weights: 3.325866300540886\n",
      "Iteration Number: 360 / Loss: 8.580436255085807 / Norm of grad_w, grad_b: 0.9698858732014501 , 0.006642262417191105 / Norm of weights: 3.3257842743344463\n",
      "Iteration Number: 361 / Loss: 8.580146270448374 / Norm of grad_w, grad_b: 0.9643434773505631 , 0.006641827734828482 / Norm of weights: 3.3257032359662886\n",
      "Iteration Number: 362 / Loss: 8.5798598033966 / Norm of grad_w, grad_b: 0.958835433674801 , 0.006641395263850242 / Norm of weights: 3.325623174374818\n",
      "Iteration Number: 363 / Loss: 8.579576814489162 / Norm of grad_w, grad_b: 0.9533614863249803 , 0.0066409649925299056 / Norm of weights: 3.3255440786244432\n",
      "Iteration Number: 364 / Loss: 8.579297264739212 / Norm of grad_w, grad_b: 0.9479213820674895 , 0.006640536909151485 / Norm of weights: 3.325465937904019\n",
      "Iteration Number: 365 / Loss: 8.579021115608619 / Norm of grad_w, grad_b: 0.9425148702459746 , 0.006640111002010478 / Norm of weights: 3.325388741525303\n",
      "Iteration Number: 366 / Loss: 8.57874832900227 / Norm of grad_w, grad_b: 0.937141702743744 , 0.006639687259414906 / Norm of weights: 3.325312478921444\n",
      "Iteration Number: 367 / Loss: 8.578478867262472 / Norm of grad_w, grad_b: 0.9318016339469404 , 0.006639265669686318 / Norm of weights: 3.325237139645488\n",
      "Iteration Number: 368 / Loss: 8.578212693163458 / Norm of grad_w, grad_b: 0.9264944207084295 , 0.006638846221160819 / Norm of weights: 3.325162713368904\n",
      "Iteration Number: 369 / Loss: 8.577949769905931 / Norm of grad_w, grad_b: 0.9212198223123148 , 0.006638428902189891 / Norm of weights: 3.3250891898801367\n",
      "Iteration Number: 370 / Loss: 8.577690061111735 / Norm of grad_w, grad_b: 0.9159776004392445 , 0.006638013701141599 / Norm of weights: 3.3250165590831746\n",
      "Iteration Number: 371 / Loss: 8.57743353081857 / Norm of grad_w, grad_b: 0.9107675191322104 , 0.0066376006064012925 / Norm of weights: 3.324944810996144\n",
      "Iteration Number: 372 / Loss: 8.57718014347483 / Norm of grad_w, grad_b: 0.9055893447631906 , 0.006637189606372562 / Norm of weights: 3.3248739357499173\n",
      "Iteration Number: 373 / Loss: 8.576929863934454 / Norm of grad_w, grad_b: 0.9004428460003018 , 0.006636780689478249 / Norm of weights: 3.324803923586748\n",
      "Iteration Number: 374 / Loss: 8.576682657451922 / Norm of grad_w, grad_b: 0.8953277937755507 , 0.006636373844161136 / Norm of weights: 3.324734764858918\n",
      "Iteration Number: 375 / Loss: 8.576438489677273 / Norm of grad_w, grad_b: 0.8902439612532697 , 0.006635969058884956 / Norm of weights: 3.3246664500274092\n",
      "Iteration Number: 376 / Loss: 8.57619732665122 / Norm of grad_w, grad_b: 0.8851911237990778 , 0.006635566322135102 / Norm of weights: 3.3245989696605927\n",
      "Iteration Number: 377 / Loss: 8.575959134800339 / Norm of grad_w, grad_b: 0.8801690589494486 , 0.006635165622419511 / Norm of weights: 3.3245323144329344\n",
      "Iteration Number: 378 / Loss: 8.575723880932312 / Norm of grad_w, grad_b: 0.8751775463818455 , 0.006634766948269463 / Norm of weights: 3.324466475123723\n",
      "Iteration Number: 379 / Loss: 8.575491532231258 / Norm of grad_w, grad_b: 0.8702163678853372 , 0.006634370288240282 / Norm of weights: 3.32440144261581\n",
      "Iteration Number: 380 / Loss: 8.575262056253104 / Norm of grad_w, grad_b: 0.8652853073318535 , 0.0066339756309121995 / Norm of weights: 3.324337207894373\n",
      "Iteration Number: 381 / Loss: 8.575035420921067 / Norm of grad_w, grad_b: 0.8603841506478885 , 0.006633582964891028 / Norm of weights: 3.3242737620456904\n",
      "Iteration Number: 382 / Loss: 8.574811594521147 / Norm of grad_w, grad_b: 0.8555126857867655 , 0.006633192278808862 / Norm of weights: 3.32421109625594\n",
      "Iteration Number: 383 / Loss: 8.574590545697733 / Norm of grad_w, grad_b: 0.8506707027013367 , 0.006632803561324916 / Norm of weights: 3.3241492018100076\n",
      "Iteration Number: 384 / Loss: 8.574372243449238 / Norm of grad_w, grad_b: 0.8458579933172299 , 0.006632416801126025 / Norm of weights: 3.324088070090314\n",
      "Iteration Number: 385 / Loss: 8.574156657123805 / Norm of grad_w, grad_b: 0.8410743515066149 , 0.00663203198692746 / Norm of weights: 3.324027692575662\n",
      "Iteration Number: 386 / Loss: 8.573943756415094 / Norm of grad_w, grad_b: 0.8363195730622854 , 0.0066316491074735785 / Norm of weights: 3.323968060840093\n",
      "Iteration Number: 387 / Loss: 8.573733511358094 / Norm of grad_w, grad_b: 0.831593455672362 , 0.006631268151538342 / Norm of weights: 3.3239091665517626\n",
      "Iteration Number: 388 / Loss: 8.573525892325023 / Norm of grad_w, grad_b: 0.8268957988953879 , 0.006630889107926085 / Norm of weights: 3.3238510014718297\n",
      "Iteration Number: 389 / Loss: 8.573320870021256 / Norm of grad_w, grad_b: 0.8222264041357933 , 0.006630511965472065 / Norm of weights: 3.323793557453364\n",
      "Iteration Number: 390 / Loss: 8.57311841548135 / Norm of grad_w, grad_b: 0.817585074619915 , 0.006630136713043053 / Norm of weights: 3.323736826440264\n",
      "Iteration Number: 391 / Loss: 8.572918500065088 / Norm of grad_w, grad_b: 0.8129716153723112 , 0.006629763339537894 / Norm of weights: 3.3236808004661906\n",
      "Iteration Number: 392 / Loss: 8.572721095453582 / Norm of grad_w, grad_b: 0.8083858331925875 , 0.006629391833888106 / Norm of weights: 3.3236254716535174\n",
      "Iteration Number: 393 / Loss: 8.572526173645462 / Norm of grad_w, grad_b: 0.8038275366325793 , 0.0066290221850583905 / Norm of weights: 3.3235708322122925\n",
      "Iteration Number: 394 / Loss: 8.572333706953074 / Norm of grad_w, grad_b: 0.7992965359739023 , 0.006628654382047223 / Norm of weights: 3.323516874439214\n",
      "Iteration Number: 395 / Loss: 8.57214366799876 / Norm of grad_w, grad_b: 0.7947926432059423 , 0.00662828841388731 / Norm of weights: 3.323463590716621\n",
      "Iteration Number: 396 / Loss: 8.571956029711174 / Norm of grad_w, grad_b: 0.7903156720042006 , 0.006627924269646115 / Norm of weights: 3.323410973511494\n",
      "Iteration Number: 397 / Loss: 8.57177076532165 / Norm of grad_w, grad_b: 0.7858654377089663 , 0.006627561938426401 / Norm of weights: 3.323359015374476\n",
      "Iteration Number: 398 / Loss: 8.571587848360636 / Norm of grad_w, grad_b: 0.7814417573044384 , 0.0066272014093665755 / Norm of weights: 3.3233077089388967\n",
      "Iteration Number: 399 / Loss: 8.571407252654147 / Norm of grad_w, grad_b: 0.7770444493981169 , 0.006626842671641283 / Norm of weights: 3.323257046919817\n",
      "Iteration Number: 400 / Loss: 8.57122895232029 / Norm of grad_w, grad_b: 0.7726733342005793 , 0.0066264857144618305 / Norm of weights: 3.3232070221130816\n",
      "Iteration Number: 401 / Loss: 8.57105292176582 / Norm of grad_w, grad_b: 0.7683282335056115 , 0.006626130527076551 / Norm of weights: 3.3231576273943864\n",
      "Iteration Number: 402 / Loss: 8.570879135682748 / Norm of grad_w, grad_b: 0.7640089706706021 , 0.0066257770987712865 / Norm of weights: 3.323108855718358\n",
      "Iteration Number: 403 / Loss: 8.570707569045013 / Norm of grad_w, grad_b: 0.7597153705973737 , 0.0066254254188698415 / Norm of weights: 3.3230607001176415\n",
      "Iteration Number: 404 / Loss: 8.570538197105153 / Norm of grad_w, grad_b: 0.7554472597132186 , 0.006625075476734213 / Norm of weights: 3.323013153702006\n",
      "Iteration Number: 405 / Loss: 8.570370995391068 / Norm of grad_w, grad_b: 0.7512044659522942 , 0.006624727261765181 / Norm of weights: 3.3229662096574546\n",
      "Iteration Number: 406 / Loss: 8.570205939702793 / Norm of grad_w, grad_b: 0.746986818737393 , 0.00662438076340258 / Norm of weights: 3.322919861245354\n",
      "Iteration Number: 407 / Loss: 8.57004300610934 / Norm of grad_w, grad_b: 0.7427941489618559 , 0.006624035971125597 / Norm of weights: 3.322874101801566\n",
      "Iteration Number: 408 / Loss: 8.569882170945547 / Norm of grad_w, grad_b: 0.7386262889719445 , 0.006623692874453267 / Norm of weights: 3.322828924735597\n",
      "Iteration Number: 409 / Loss: 8.569723410809004 / Norm of grad_w, grad_b: 0.7344830725493453 , 0.006623351462944699 / Norm of weights: 3.3227843235297563\n",
      "Iteration Number: 410 / Loss: 8.569566702556996 / Norm of grad_w, grad_b: 0.730364334894126 , 0.006623011726199415 / Norm of weights: 3.322740291738325\n",
      "Iteration Number: 411 / Loss: 8.569412023303503 / Norm of grad_w, grad_b: 0.7262699126078168 , 0.006622673653857706 / Norm of weights: 3.3226968229867326\n",
      "Iteration Number: 412 / Loss: 8.569259350416209 / Norm of grad_w, grad_b: 0.7221996436768173 , 0.006622337235600911 / Norm of weights: 3.3226539109707502\n",
      "Iteration Number: 413 / Loss: 8.569108661513598 / Norm of grad_w, grad_b: 0.7181533674561265 , 0.0066220024611517055 / Norm of weights: 3.322611549455686\n",
      "Iteration Number: 414 / Loss: 8.568959934462033 / Norm of grad_w, grad_b: 0.7141309246532084 , 0.006621669320274417 / Norm of weights: 3.3225697322756\n",
      "Iteration Number: 415 / Loss: 8.568813147372916 / Norm of grad_w, grad_b: 0.7101321573122793 , 0.006621337802775176 / Norm of weights: 3.3225284533325183\n",
      "Iteration Number: 416 / Loss: 8.568668278599866 / Norm of grad_w, grad_b: 0.7061569087986295 , 0.006621007898502358 / Norm of weights: 3.3224877065956657\n",
      "Iteration Number: 417 / Loss: 8.56852530673592 / Norm of grad_w, grad_b: 0.7022050237834081 , 0.0066206795973466695 / Norm of weights: 3.3224474861007045\n",
      "Iteration Number: 418 / Loss: 8.568384210610803 / Norm of grad_w, grad_b: 0.6982763482285154 , 0.006620352889241511 / Norm of weights: 3.3224077859489816\n",
      "Iteration Number: 419 / Loss: 8.568244969288209 / Norm of grad_w, grad_b: 0.6943707293717406 , 0.006620027764163138 / Norm of weights: 3.3223686003067856\n",
      "Iteration Number: 420 / Loss: 8.56810756206311 / Norm of grad_w, grad_b: 0.6904880157122018 , 0.006619704212130912 / Norm of weights: 3.3223299234046157\n",
      "Iteration Number: 421 / Loss: 8.567971968459128 / Norm of grad_w, grad_b: 0.6866280569959347 , 0.006619382223207419 / Norm of weights: 3.322291749536455\n",
      "Iteration Number: 422 / Loss: 8.567838168225908 / Norm of grad_w, grad_b: 0.6827907042017424 , 0.006619061787498824 / Norm of weights: 3.322254073059058\n",
      "Iteration Number: 423 / Loss: 8.567706141336554 / Norm of grad_w, grad_b: 0.6789758095272435 , 0.006618742895154974 / Norm of weights: 3.322216888391241\n",
      "Iteration Number: 424 / Loss: 8.567575867985067 / Norm of grad_w, grad_b: 0.6751832263751674 , 0.006618425536369546 / Norm of weights: 3.3221801900131855\n",
      "Iteration Number: 425 / Loss: 8.567447328583835 / Norm of grad_w, grad_b: 0.671412809339819 , 0.006618109701380345 / Norm of weights: 3.3221439724657515\n",
      "Iteration Number: 426 / Loss: 8.567320503761167 / Norm of grad_w, grad_b: 0.667664414193777 , 0.00661779538046927 / Norm of weights: 3.3221082303497904\n",
      "Iteration Number: 427 / Loss: 8.567195374358809 / Norm of grad_w, grad_b: 0.6639378978747736 , 0.006617482563962764 / Norm of weights: 3.322072958325477\n",
      "Iteration Number: 428 / Loss: 8.567071921429548 / Norm of grad_w, grad_b: 0.6602331184727868 , 0.006617171242231577 / Norm of weights: 3.322038151111645\n",
      "Iteration Number: 429 / Loss: 8.56695012623481 / Norm of grad_w, grad_b: 0.6565499352173263 , 0.006616861405691285 / Norm of weights: 3.322003803485128\n",
      "Iteration Number: 430 / Loss: 8.566829970242303 / Norm of grad_w, grad_b: 0.6528882084649095 , 0.006616553044802178 / Norm of weights: 3.3219699102801106\n",
      "Iteration Number: 431 / Loss: 8.566711435123672 / Norm of grad_w, grad_b: 0.649247799686704 , 0.006616246150069495 / Norm of weights: 3.3219364663874917\n",
      "Iteration Number: 432 / Loss: 8.566594502752215 / Norm of grad_w, grad_b: 0.6456285714563771 , 0.006615940712043447 / Norm of weights: 3.3219034667542466\n",
      "Iteration Number: 433 / Loss: 8.56647915520059 / Norm of grad_w, grad_b: 0.6420303874381423 , 0.00661563672131944 / Norm of weights: 3.3218709063828036\n",
      "Iteration Number: 434 / Loss: 8.566365374738567 / Norm of grad_w, grad_b: 0.6384531123749162 , 0.00661533416853805 / Norm of weights: 3.3218387803304266\n",
      "Iteration Number: 435 / Loss: 8.566253143830828 / Norm of grad_w, grad_b: 0.6348966120767267 , 0.006615033044385286 / Norm of weights: 3.3218070837086033\n",
      "Iteration Number: 436 / Loss: 8.56614244513475 / Norm of grad_w, grad_b: 0.6313607534092518 , 0.006614733339592435 / Norm of weights: 3.321775811682442\n",
      "Iteration Number: 437 / Loss: 8.566033261498252 / Norm of grad_w, grad_b: 0.6278454042825046 , 0.006614435044936422 / Norm of weights: 3.321744959470075\n",
      "Iteration Number: 438 / Loss: 8.565925575957653 / Norm of grad_w, grad_b: 0.6243504336397445 , 0.006614138151239553 / Norm of weights: 3.3217145223420714\n",
      "Iteration Number: 439 / Loss: 8.56581937173555 / Norm of grad_w, grad_b: 0.6208757114464848 , 0.006613842649370007 / Norm of weights: 3.3216844956208527\n",
      "Iteration Number: 440 / Loss: 8.565714632238743 / Norm of grad_w, grad_b: 0.6174211086797099 , 0.0066135485302414145 / Norm of weights: 3.321654874680118\n",
      "Iteration Number: 441 / Loss: 8.565611341056163 / Norm of grad_w, grad_b: 0.6139864973171977 , 0.006613255784813294 / Norm of weights: 3.321625654944278\n",
      "Iteration Number: 442 / Loss: 8.565509481956838 / Norm of grad_w, grad_b: 0.6105717503270457 , 0.006612964404090864 / Norm of weights: 3.3215968318878875\n",
      "Iteration Number: 443 / Loss: 8.565409038887868 / Norm of grad_w, grad_b: 0.607176741657325 , 0.00661267437912519 / Norm of weights: 3.3215684010350977\n",
      "Iteration Number: 444 / Loss: 8.565309995972452 / Norm of grad_w, grad_b: 0.6038013462258535 , 0.006612385701013236 / Norm of weights: 3.3215403579591025\n",
      "Iteration Number: 445 / Loss: 8.565212337507921 / Norm of grad_w, grad_b: 0.600445439910154 , 0.006612098360897733 / Norm of weights: 3.3215126982815972\n",
      "Iteration Number: 446 / Loss: 8.565116047963777 / Norm of grad_w, grad_b: 0.5971088995375772 , 0.006611812349967456 / Norm of weights: 3.321485417672244\n",
      "Iteration Number: 447 / Loss: 8.565021111979796 / Norm of grad_w, grad_b: 0.593791602875437 , 0.006611527659456961 / Norm of weights: 3.3214585118481397\n",
      "Iteration Number: 448 / Loss: 8.564927514364113 / Norm of grad_w, grad_b: 0.5904934286214738 , 0.006611244280646822 / Norm of weights: 3.3214319765732974\n",
      "Iteration Number: 449 / Loss: 8.564835240091373 / Norm of grad_w, grad_b: 0.5872142563942835 , 0.006610962204863471 / Norm of weights: 3.3214058076581225\n",
      "Iteration Number: 450 / Loss: 8.564744274300855 / Norm of grad_w, grad_b: 0.5839539667239708 , 0.006610681423479305 / Norm of weights: 3.321380000958908\n",
      "Iteration Number: 451 / Loss: 8.564654602294656 / Norm of grad_w, grad_b: 0.5807124410429086 , 0.006610401927912629 / Norm of weights: 3.3213545523773242\n",
      "Iteration Number: 452 / Loss: 8.564566209535881 / Norm of grad_w, grad_b: 0.5774895616766237 , 0.006610123709627611 / Norm of weights: 3.321329457859921\n",
      "Iteration Number: 453 / Loss: 8.564479081646855 / Norm of grad_w, grad_b: 0.5742852118347928 , 0.006609846760134332 / Norm of weights: 3.3213047133976357\n",
      "Iteration Number: 454 / Loss: 8.564393204407365 / Norm of grad_w, grad_b: 0.5710992756024229 , 0.006609571070988769 / Norm of weights: 3.3212803150253025\n",
      "Iteration Number: 455 / Loss: 8.5643085637529 / Norm of grad_w, grad_b: 0.5679316379310291 , 0.006609296633792697 / Norm of weights: 3.3212562588211716\n",
      "Iteration Number: 456 / Loss: 8.56422514577295 / Norm of grad_w, grad_b: 0.564782184630115 , 0.006609023440193617 / Norm of weights: 3.3212325409064314\n",
      "Iteration Number: 457 / Loss: 8.564142936709278 / Norm of grad_w, grad_b: 0.5616508023585302 , 0.0066087514818849264 / Norm of weights: 3.321209157444739\n",
      "Iteration Number: 458 / Loss: 8.564061922954252 / Norm of grad_w, grad_b: 0.5585373786162402 , 0.006608480750605637 / Norm of weights: 3.3211861046417526\n",
      "Iteration Number: 459 / Loss: 8.563982091049184 / Norm of grad_w, grad_b: 0.5554418017358939 , 0.006608211238140457 / Norm of weights: 3.321163378744672\n",
      "Iteration Number: 460 / Loss: 8.563903427682671 / Norm of grad_w, grad_b: 0.5523639608747586 , 0.0066079429363197495 / Norm of weights: 3.321140976041782\n",
      "Iteration Number: 461 / Loss: 8.56382591968898 / Norm of grad_w, grad_b: 0.5493037460066018 , 0.006607675837019406 / Norm of weights: 3.321118892862004\n",
      "Iteration Number: 462 / Loss: 8.563749554046444 / Norm of grad_w, grad_b: 0.5462610479137701 , 0.0066074099321608275 / Norm of weights: 3.32109712557445\n",
      "Iteration Number: 463 / Loss: 8.563674317875861 / Norm of grad_w, grad_b: 0.5432357581793373 , 0.0066071452137108615 / Norm of weights: 3.3210756705879834\n",
      "Iteration Number: 464 / Loss: 8.563600198438941 / Norm of grad_w, grad_b: 0.5402277691793413 , 0.006606881673681714 / Norm of weights: 3.321054524350783\n",
      "Iteration Number: 465 / Loss: 8.563527183136742 / Norm of grad_w, grad_b: 0.537236974075166 , 0.006606619304130897 / Norm of weights: 3.3210336833499157\n",
      "Iteration Number: 466 / Loss: 8.563455259508148 / Norm of grad_w, grad_b: 0.534263266805993 , 0.006606358097161177 / Norm of weights: 3.3210131441109074\n",
      "Iteration Number: 467 / Loss: 8.56338441522834 / Norm of grad_w, grad_b: 0.5313065420813572 , 0.006606098044920387 / Norm of weights: 3.320992903197328\n",
      "Iteration Number: 468 / Loss: 8.563314638107315 / Norm of grad_w, grad_b: 0.528366695373803 , 0.006605839139601509 / Norm of weights: 3.320972957210372\n",
      "Iteration Number: 469 / Loss: 8.563245916088395 / Norm of grad_w, grad_b: 0.5254436229116524 , 0.006605581373442455 / Norm of weights: 3.320953302788449\n",
      "Iteration Number: 470 / Loss: 8.563178237246758 / Norm of grad_w, grad_b: 0.5225372216718002 , 0.006605324738726008 / Norm of weights: 3.3209339366067776\n",
      "Iteration Number: 471 / Loss: 8.563111589788006 / Norm of grad_w, grad_b: 0.519647389372693 , 0.0066050692277797975 / Norm of weights: 3.320914855376986\n",
      "Iteration Number: 472 / Loss: 8.563045962046726 / Norm of grad_w, grad_b: 0.5167740244673584 , 0.006604814832976083 / Norm of weights: 3.3208960558467107\n",
      "Iteration Number: 473 / Loss: 8.562981342485072 / Norm of grad_w, grad_b: 0.5139170261364996 , 0.006604561546731718 / Norm of weights: 3.3208775347992088\n",
      "Iteration Number: 474 / Loss: 8.56291771969138 / Norm of grad_w, grad_b: 0.5110762942816899 , 0.006604309361508136 / Norm of weights: 3.320859289052966\n",
      "Iteration Number: 475 / Loss: 8.562855082378775 / Norm of grad_w, grad_b: 0.5082517295187071 , 0.0066040582698110185 / Norm of weights: 3.3208413154613177\n",
      "Iteration Number: 476 / Loss: 8.562793419383812 / Norm of grad_w, grad_b: 0.5054432331708686 , 0.006603808264190442 / Norm of weights: 3.3208236109120635\n",
      "Iteration Number: 477 / Loss: 8.56273271966512 / Norm of grad_w, grad_b: 0.5026507072625079 , 0.006603559337240573 / Norm of weights: 3.320806172327097\n",
      "Iteration Number: 478 / Loss: 8.562672972302074 / Norm of grad_w, grad_b: 0.4998740545124992 , 0.006603311481599644 / Norm of weights: 3.3207889966620345\n",
      "Iteration Number: 479 / Loss: 8.562614166493475 / Norm of grad_w, grad_b: 0.49711317832790564 , 0.006603064689949861 / Norm of weights: 3.320772080905844\n",
      "Iteration Number: 480 / Loss: 8.562556291556229 / Norm of grad_w, grad_b: 0.49436798279766814 , 0.006602818955017122 / Norm of weights: 3.3207554220804893\n",
      "Iteration Number: 481 / Loss: 8.562499336924086 / Norm of grad_w, grad_b: 0.4916383726863718 , 0.006602574269571133 / Norm of weights: 3.3207390172405655\n",
      "Iteration Number: 482 / Loss: 8.562443292146337 / Norm of grad_w, grad_b: 0.48892425342813367 , 0.006602330626425125 / Norm of weights: 3.3207228634729478\n",
      "Iteration Number: 483 / Loss: 8.562388146886574 / Norm of grad_w, grad_b: 0.48622553112048555 , 0.0066020880184356765 / Norm of weights: 3.3207069578964403\n",
      "Iteration Number: 484 / Loss: 8.562333890921431 / Norm of grad_w, grad_b: 0.48354211251846224 , 0.006601846438502803 / Norm of weights: 3.320691297661428\n",
      "Iteration Number: 485 / Loss: 8.562280514139356 / Norm of grad_w, grad_b: 0.4808739050285774 , 0.0066016058795695855 / Norm of weights: 3.3206758799495355\n",
      "Iteration Number: 486 / Loss: 8.562228006539389 / Norm of grad_w, grad_b: 0.4782208167030836 , 0.006601366334622183 / Norm of weights: 3.320660701973288\n",
      "Iteration Number: 487 / Loss: 8.562176358229962 / Norm of grad_w, grad_b: 0.4755827562341098 , 0.006601127796689627 / Norm of weights: 3.320645760975775\n",
      "Iteration Number: 488 / Loss: 8.56212555942771 / Norm of grad_w, grad_b: 0.4729596329480119 , 0.006600890258843759 / Norm of weights: 3.3206310542303195\n",
      "Iteration Number: 489 / Loss: 8.562075600456279 / Norm of grad_w, grad_b: 0.4703513567997298 , 0.006600653714198927 / Norm of weights: 3.32061657904015\n",
      "Iteration Number: 490 / Loss: 8.562026471745181 / Norm of grad_w, grad_b: 0.46775783836716106 , 0.006600418155912061 / Norm of weights: 3.320602332738076\n",
      "Iteration Number: 491 / Loss: 8.561978163828623 / Norm of grad_w, grad_b: 0.46517898884575093 , 0.006600183577182335 / Norm of weights: 3.320588312686169\n",
      "Iteration Number: 492 / Loss: 8.561930667344393 / Norm of grad_w, grad_b: 0.46261472004299115 , 0.006599949971251168 / Norm of weights: 3.320574516275442\n",
      "Iteration Number: 493 / Loss: 8.5618839730327 / Norm of grad_w, grad_b: 0.4600649443730718 , 0.00659971733140195 / Norm of weights: 3.320560940925541\n",
      "Iteration Number: 494 / Loss: 8.561838071735092 / Norm of grad_w, grad_b: 0.4575295748515688 , 0.006599485650959987 / Norm of weights: 3.320547584084429\n",
      "Iteration Number: 495 / Loss: 8.56179295439334 / Norm of grad_w, grad_b: 0.45500852509019907 , 0.0065992549232922575 / Norm of weights: 3.3205344432280848\n",
      "Iteration Number: 496 / Loss: 8.561748612048353 / Norm of grad_w, grad_b: 0.4525017092916351 , 0.0065990251418074145 / Norm of weights: 3.3205215158601966\n",
      "Iteration Number: 497 / Loss: 8.5617050358391 / Norm of grad_w, grad_b: 0.4500090422444082 , 0.0065987962999553935 / Norm of weights: 3.3205087995118623\n",
      "Iteration Number: 498 / Loss: 8.561662217001548 / Norm of grad_w, grad_b: 0.44753043931779385 , 0.006598568391227459 / Norm of weights: 3.3204962917412937\n",
      "Iteration Number: 499 / Loss: 8.5616201468676 / Norm of grad_w, grad_b: 0.44506581645689003 , 0.006598341409156022 / Norm of weights: 3.3204839901335235\n",
      "Iteration Number: 500 / Loss: 8.56157881686408 / Norm of grad_w, grad_b: 0.4426150901775709 , 0.006598115347314281 / Norm of weights: 3.3204718923001133\n",
      "Iteration Number: 501 / Loss: 8.561538218511664 / Norm of grad_w, grad_b: 0.44017817756170136 , 0.006597890199316372 / Norm of weights: 3.3204599958788696\n",
      "Iteration Number: 502 / Loss: 8.561498343423903 / Norm of grad_w, grad_b: 0.43775499625222264 , 0.006597665958816966 / Norm of weights: 3.3204482985335555\n",
      "Iteration Number: 503 / Loss: 8.56145918330618 / Norm of grad_w, grad_b: 0.4353454644484379 , 0.00659744261951113 / Norm of weights: 3.320436797953616\n",
      "Iteration Number: 504 / Loss: 8.561420729954747 / Norm of grad_w, grad_b: 0.43294950090126016 , 0.006597220175134325 / Norm of weights: 3.3204254918538947\n",
      "Iteration Number: 505 / Loss: 8.561382975255718 / Norm of grad_w, grad_b: 0.4305670249085363 , 0.006596998619462101 / Norm of weights: 3.320414377974364\n",
      "Iteration Number: 506 / Loss: 8.561345911184107 / Norm of grad_w, grad_b: 0.42819795631050034 , 0.006596777946309846 / Norm of weights: 3.320403454079849\n",
      "Iteration Number: 507 / Loss: 8.561309529802855 / Norm of grad_w, grad_b: 0.425842215485093 , 0.006596558149532843 / Norm of weights: 3.320392717959763\n",
      "Iteration Number: 508 / Loss: 8.561273823261894 / Norm of grad_w, grad_b: 0.4234997233435952 , 0.006596339223025955 / Norm of weights: 3.32038216742784\n",
      "Iteration Number: 509 / Loss: 8.561238783797188 / Norm of grad_w, grad_b: 0.42117040132604455 , 0.006596121160723452 / Norm of weights: 3.320371800321871\n",
      "Iteration Number: 510 / Loss: 8.56120440372981 / Norm of grad_w, grad_b: 0.41885417139691145 , 0.0065959039565989235 / Norm of weights: 3.320361614503446\n",
      "Iteration Number: 511 / Loss: 8.561170675465029 / Norm of grad_w, grad_b: 0.4165509560406796 , 0.006595687604664958 / Norm of weights: 3.3203516078576962\n",
      "Iteration Number: 512 / Loss: 8.561137591491379 / Norm of grad_w, grad_b: 0.414260678257617 , 0.006595472098973195 / Norm of weights: 3.3203417782930402\n",
      "Iteration Number: 513 / Loss: 8.56110514437978 / Norm of grad_w, grad_b: 0.41198326155940307 , 0.006595257433613869 / Norm of weights: 3.3203321237409313\n",
      "Iteration Number: 514 / Loss: 8.561073326782637 / Norm of grad_w, grad_b: 0.4097186299650005 , 0.00659504360271589 / Norm of weights: 3.32032264215561\n",
      "Iteration Number: 515 / Loss: 8.561042131432952 / Norm of grad_w, grad_b: 0.40746670799646695 , 0.006594830600446461 / Norm of weights: 3.320313331513858\n",
      "Iteration Number: 516 / Loss: 8.561011551143467 / Norm of grad_w, grad_b: 0.4052274206748028 , 0.006594618421011168 / Norm of weights: 3.3203041898147547\n",
      "Iteration Number: 517 / Loss: 8.560981578805794 / Norm of grad_w, grad_b: 0.40300069351592543 , 0.006594407058653383 / Norm of weights: 3.3202952150794363\n",
      "Iteration Number: 518 / Loss: 8.560952207389564 / Norm of grad_w, grad_b: 0.4007864525265648 , 0.006594196507654583 / Norm of weights: 3.3202864053508567\n",
      "Iteration Number: 519 / Loss: 8.56092342994158 / Norm of grad_w, grad_b: 0.39858462420036356 , 0.006593986762333768 / Norm of weights: 3.3202777586935555\n",
      "Iteration Number: 520 / Loss: 8.560895239584998 / Norm of grad_w, grad_b: 0.3963951355138432 , 0.006593777817047483 / Norm of weights: 3.3202692731934214\n",
      "Iteration Number: 521 / Loss: 8.56086762951848 / Norm of grad_w, grad_b: 0.39421791392255146 , 0.0065935696661895745 / Norm of weights: 3.320260946957463\n",
      "Iteration Number: 522 / Loss: 8.5608405930154 / Norm of grad_w, grad_b: 0.39205288735721117 , 0.006593362304191102 / Norm of weights: 3.3202527781135815\n",
      "Iteration Number: 523 / Loss: 8.560814123423018 / Norm of grad_w, grad_b: 0.38989998421986516 , 0.006593155725519966 / Norm of weights: 3.320244764810346\n",
      "Iteration Number: 524 / Loss: 8.5607882141617 / Norm of grad_w, grad_b: 0.3877591333801214 , 0.0065929499246808595 / Norm of weights: 3.320236905216768\n",
      "Iteration Number: 525 / Loss: 8.56076285872412 / Norm of grad_w, grad_b: 0.38563026417140256 , 0.006592744896215189 / Norm of weights: 3.320229197522084\n",
      "Iteration Number: 526 / Loss: 8.560738050674473 / Norm of grad_w, grad_b: 0.3835133063872509 , 0.0065925406347006705 / Norm of weights: 3.3202216399355335\n",
      "Iteration Number: 527 / Loss: 8.560713783647715 / Norm of grad_w, grad_b: 0.3814081902776776 , 0.006592337134751206 / Norm of weights: 3.3202142306861484\n",
      "Iteration Number: 528 / Loss: 8.560690051348796 / Norm of grad_w, grad_b: 0.37931484654552 , 0.006592134391016868 / Norm of weights: 3.320206968022535\n",
      "Iteration Number: 529 / Loss: 8.560666847551905 / Norm of grad_w, grad_b: 0.3772332063429165 , 0.0065919323981834844 / Norm of weights: 3.3201998502126635\n",
      "Iteration Number: 530 / Loss: 8.560644166099713 / Norm of grad_w, grad_b: 0.37516320126767466 , 0.006591731150972599 / Norm of weights: 3.3201928755436625\n",
      "Iteration Number: 531 / Loss: 8.56062200090265 / Norm of grad_w, grad_b: 0.3731047633598238 , 0.006591530644141293 / Norm of weights: 3.320186042321609\n",
      "Iteration Number: 532 / Loss: 8.560600345938166 / Norm of grad_w, grad_b: 0.37105782509814145 , 0.006591330872481872 / Norm of weights: 3.3201793488713243\n",
      "Iteration Number: 533 / Loss: 8.560579195250002 / Norm of grad_w, grad_b: 0.3690223193967073 , 0.006591131830821844 / Norm of weights: 3.3201727935361736\n",
      "Iteration Number: 534 / Loss: 8.560558542947486 / Norm of grad_w, grad_b: 0.3669981796014749 , 0.006590933514023623 / Norm of weights: 3.3201663746778656\n",
      "Iteration Number: 535 / Loss: 8.560538383204829 / Norm of grad_w, grad_b: 0.36498533948697237 , 0.006590735916984367 / Norm of weights: 3.3201600906762523\n",
      "Iteration Number: 536 / Loss: 8.5605187102604 / Norm of grad_w, grad_b: 0.36298373325290556 , 0.006590539034635837 / Norm of weights: 3.320153939929137\n",
      "Iteration Number: 537 / Loss: 8.560499518416071 / Norm of grad_w, grad_b: 0.36099329552091974 , 0.00659034286194417 / Norm of weights: 3.320147920852078\n",
      "Iteration Number: 538 / Loss: 8.560480802036496 / Norm of grad_w, grad_b: 0.3590139613312948 , 0.0065901473939097245 / Norm of weights: 3.320142031878199\n",
      "Iteration Number: 539 / Loss: 8.560462555548465 / Norm of grad_w, grad_b: 0.3570456661397406 , 0.006589952625566838 / Norm of weights: 3.320136271457997\n",
      "Iteration Number: 540 / Loss: 8.560444773440214 / Norm of grad_w, grad_b: 0.3550883458141807 , 0.006589758551983732 / Norm of weights: 3.320130638059158\n",
      "Iteration Number: 541 / Loss: 8.560427450260764 / Norm of grad_w, grad_b: 0.353141936631628 , 0.006589565168262238 / Norm of weights: 3.32012513016637\n",
      "Iteration Number: 542 / Loss: 8.560410580619282 / Norm of grad_w, grad_b: 0.35120637527499726 , 0.006589372469537689 / Norm of weights: 3.32011974628114\n",
      "Iteration Number: 543 / Loss: 8.56039415918441 / Norm of grad_w, grad_b: 0.3492815988300517 , 0.0065891804509786555 / Norm of weights: 3.320114484921613\n",
      "Iteration Number: 544 / Loss: 8.560378180683644 / Norm of grad_w, grad_b: 0.3473675447823189 , 0.006588989107786861 / Norm of weights: 3.3201093446223897\n",
      "Iteration Number: 545 / Loss: 8.560362639902683 / Norm of grad_w, grad_b: 0.34546415101404476 , 0.006588798435196883 / Norm of weights: 3.320104323934355\n",
      "Iteration Number: 546 / Loss: 8.560347531684819 / Norm of grad_w, grad_b: 0.3435713558011851 , 0.006588608428476087 / Norm of weights: 3.320099421424496\n",
      "Iteration Number: 547 / Loss: 8.560332850930294 / Norm of grad_w, grad_b: 0.34168909781046886 , 0.006588419082924327 / Norm of weights: 3.3200946356757326\n",
      "Iteration Number: 548 / Loss: 8.560318592595712 / Norm of grad_w, grad_b: 0.3398173160963672 , 0.006588230393873884 / Norm of weights: 3.3200899652867433\n",
      "Iteration Number: 549 / Loss: 8.560304751693408 / Norm of grad_w, grad_b: 0.33795595009824686 , 0.0065880423566891284 / Norm of weights: 3.3200854088717966\n",
      "Iteration Number: 550 / Loss: 8.560291323290866 / Norm of grad_w, grad_b: 0.33610493963746374 , 0.006587854966766516 / Norm of weights: 3.3200809650605816\n",
      "Iteration Number: 551 / Loss: 8.560278302510106 / Norm of grad_w, grad_b: 0.3342642249144767 , 0.006587668219534282 / Norm of weights: 3.320076632498043\n",
      "Iteration Number: 552 / Loss: 8.560265684527119 / Norm of grad_w, grad_b: 0.3324337465060191 , 0.006587482110452249 / Norm of weights: 3.320072409844216\n",
      "Iteration Number: 553 / Loss: 8.560253464571268 / Norm of grad_w, grad_b: 0.3306134453623229 , 0.006587296635011753 / Norm of weights: 3.3200682957740617\n",
      "Iteration Number: 554 / Loss: 8.56024163792471 / Norm of grad_w, grad_b: 0.32880326280429373 , 0.006587111788735347 / Norm of weights: 3.3200642889773095\n",
      "Iteration Number: 555 / Loss: 8.560230199921847 / Norm of grad_w, grad_b: 0.3270031405208052 , 0.00658692756717666 / Norm of weights: 3.3200603881582955\n",
      "Iteration Number: 556 / Loss: 8.560219145948743 / Norm of grad_w, grad_b: 0.32521302056591084 , 0.0065867439659202795 / Norm of weights: 3.320056592035805\n",
      "Iteration Number: 557 / Loss: 8.560208471442582 / Norm of grad_w, grad_b: 0.3234328453562297 , 0.006586560980581468 / Norm of weights: 3.320052899342916\n",
      "Iteration Number: 558 / Loss: 8.560198171891102 / Norm of grad_w, grad_b: 0.3216625576681872 , 0.006586378606806021 / Norm of weights: 3.3200493088268472\n",
      "Iteration Number: 559 / Loss: 8.560188242832059 / Norm of grad_w, grad_b: 0.31990210063541563 , 0.00658619684027011 / Norm of weights: 3.320045819248803\n",
      "Iteration Number: 560 / Loss: 8.56017867985269 / Norm of grad_w, grad_b: 0.3181514177461182 , 0.006586015676680084 / Norm of weights: 3.320042429383824\n",
      "Iteration Number: 561 / Loss: 8.56016947858918 / Norm of grad_w, grad_b: 0.3164104528404729 , 0.006585835111772279 / Norm of weights: 3.320039138020637\n",
      "Iteration Number: 562 / Loss: 8.560160634726127 / Norm of grad_w, grad_b: 0.3146791501080521 , 0.006585655141312872 / Norm of weights: 3.320035943961507\n",
      "Iteration Number: 563 / Loss: 8.56015214399602 / Norm of grad_w, grad_b: 0.31295745408528824 , 0.006585475761097636 / Norm of weights: 3.320032846022092\n",
      "Iteration Number: 564 / Loss: 8.560144002178735 / Norm of grad_w, grad_b: 0.31124530965292285 , 0.006585296966951849 / Norm of weights: 3.3200298430312962\n",
      "Iteration Number: 565 / Loss: 8.560136205101012 / Norm of grad_w, grad_b: 0.3095426620335315 , 0.00658511875473006 / Norm of weights: 3.320026933831131\n",
      "Iteration Number: 566 / Loss: 8.560128748635954 / Norm of grad_w, grad_b: 0.30784945678902953 , 0.006584941120315899 / Norm of weights: 3.3200241172765685\n",
      "Iteration Number: 567 / Loss: 8.560121628702525 / Norm of grad_w, grad_b: 0.3061656398182328 , 0.006584764059621954 / Norm of weights: 3.3200213922354056\n",
      "Iteration Number: 568 / Loss: 8.560114841265065 / Norm of grad_w, grad_b: 0.3044911573543945 , 0.006584587568589527 / Norm of weights: 3.3200187575881235\n",
      "Iteration Number: 569 / Loss: 8.560108382332787 / Norm of grad_w, grad_b: 0.302825955962821 , 0.006584411643188598 / Norm of weights: 3.3200162122277495\n",
      "Iteration Number: 570 / Loss: 8.5601022479593 / Norm of grad_w, grad_b: 0.3011699825384969 , 0.006584236279417386 / Norm of weights: 3.3200137550597244\n",
      "Iteration Number: 571 / Loss: 8.56009643424213 / Norm of grad_w, grad_b: 0.29952318430367547 , 0.006584061473302439 / Norm of weights: 3.320011385001767\n",
      "Iteration Number: 572 / Loss: 8.56009093732225 / Norm of grad_w, grad_b: 0.29788550880556336 , 0.006583887220898354 / Norm of weights: 3.3200091009837402\n",
      "Iteration Number: 573 / Loss: 8.560085753383603 / Norm of grad_w, grad_b: 0.29625690391401527 , 0.006583713518287575 / Norm of weights: 3.320006901947521\n",
      "Iteration Number: 574 / Loss: 8.560080878652647 / Norm of grad_w, grad_b: 0.29463731781918406 , 0.006583540361580247 / Norm of weights: 3.3200047868468707\n",
      "Iteration Number: 575 / Loss: 8.560076309397887 / Norm of grad_w, grad_b: 0.29302669902928097 , 0.0065833677469140815 / Norm of weights: 3.320002754647306\n",
      "Iteration Number: 576 / Loss: 8.560072041929434 / Norm of grad_w, grad_b: 0.291424996368315 , 0.006583195670454075 / Norm of weights: 3.3200008043259728\n",
      "Iteration Number: 577 / Loss: 8.560068072598543 / Norm of grad_w, grad_b: 0.289832158973839 , 0.006583024128392492 / Norm of weights: 3.3199989348715175\n",
      "Iteration Number: 578 / Loss: 8.56006439779718 / Norm of grad_w, grad_b: 0.28824813629471935 , 0.006582853116948516 / Norm of weights: 3.3199971452839674\n",
      "Iteration Number: 579 / Loss: 8.56006101395758 / Norm of grad_w, grad_b: 0.28667287808896486 , 0.0065826826323682865 / Norm of weights: 3.3199954345746026\n",
      "Iteration Number: 580 / Loss: 8.560057917551797 / Norm of grad_w, grad_b: 0.2851063344215149 , 0.006582512670924527 / Norm of weights: 3.3199938017658384\n",
      "Iteration Number: 581 / Loss: 8.560055105091305 / Norm of grad_w, grad_b: 0.2835484556621375 , 0.006582343228916506 / Norm of weights: 3.3199922458911013\n",
      "Iteration Number: 582 / Loss: 8.560052573126544 / Norm of grad_w, grad_b: 0.28199919248322414 , 0.006582174302669798 / Norm of weights: 3.3199907659947137\n",
      "Iteration Number: 583 / Loss: 8.560050318246518 / Norm of grad_w, grad_b: 0.28045849585770366 , 0.00658200588853617 / Norm of weights: 3.3199893611317717\n",
      "Iteration Number: 584 / Loss: 8.560048337078367 / Norm of grad_w, grad_b: 0.27892631705692056 , 0.006581837982893342 / Norm of weights: 3.3199880303680316\n",
      "Iteration Number: 585 / Loss: 8.560046626286958 / Norm of grad_w, grad_b: 0.27740260764858277 , 0.006581670582144953 / Norm of weights: 3.319986772779793\n",
      "Iteration Number: 586 / Loss: 8.560045182574484 / Norm of grad_w, grad_b: 0.27588731949464784 , 0.006581503682720235 / Norm of weights: 3.3199855874537865\n",
      "Iteration Number: 587 / Loss: 8.560044002680055 / Norm of grad_w, grad_b: 0.2743804047493433 , 0.006581337281073914 / Norm of weights: 3.319984473487057\n",
      "Iteration Number: 588 / Loss: 8.560043083379298 / Norm of grad_w, grad_b: 0.27288181585705107 , 0.006581171373686093 / Norm of weights: 3.3199834299868565\n",
      "Iteration Number: 589 / Loss: 8.56004242148397 / Norm of grad_w, grad_b: 0.27139150555037134 , 0.006581005957062035 / Norm of weights: 3.31998245607053\n",
      "Iteration Number: 590 / Loss: 8.560042013841562 / Norm of grad_w, grad_b: 0.26990942684809377 , 0.006580841027731985 / Norm of weights: 3.3199815508654074\n",
      "Iteration Number: 591 / Loss: 8.560041857334905 / Norm of grad_w, grad_b: 0.26843553305321577 , 0.006580676582251023 / Norm of weights: 3.3199807135086963\n",
      "Iteration Number: 592 / Loss: 8.560041948881812 / Norm of grad_w, grad_b: 0.2669697777509961 , 0.006580512617198975 / Norm of weights: 3.3199799431473727\n",
      "Iteration Number: 593 / Loss: 8.560042285434672 / Norm of grad_w, grad_b: 0.265512114807018 , 0.0065803491291801346 / Norm of weights: 3.3199792389380782\n",
      "Iteration Number: 594 / Loss: 8.5600428639801 / Norm of grad_w, grad_b: 0.2640624983652519 , 0.006580186114823156 / Norm of weights: 3.319978600047011\n",
      "Iteration Number: 595 / Loss: 8.560043681538545 / Norm of grad_w, grad_b: 0.26262088284613333 , 0.006580023570780892 / Norm of weights: 3.319978025649827\n",
      "Iteration Number: 596 / Loss: 8.560044735163949 / Norm of grad_w, grad_b: 0.26118722294469654 , 0.006579861493730233 / Norm of weights: 3.3199775149315323\n",
      "Iteration Number: 597 / Loss: 8.560046021943357 / Norm of grad_w, grad_b: 0.25976147362868984 , 0.006579699880371984 / Norm of weights: 3.3199770670863873\n",
      "Iteration Number: 598 / Loss: 8.560047538996589 / Norm of grad_w, grad_b: 0.2583435901366956 , 0.006579538727430623 / Norm of weights: 3.3199766813178\n",
      "Iteration Number: 599 / Loss: 8.560049283475852 / Norm of grad_w, grad_b: 0.2569335279763082 , 0.006579378031654207 / Norm of weights: 3.3199763568382332\n",
      "Iteration Number: 600 / Loss: 8.560051252565431 / Norm of grad_w, grad_b: 0.2555312429223027 , 0.006579217789814191 / Norm of weights: 3.3199760928691\n",
      "Iteration Number: 601 / Loss: 8.560053443481298 / Norm of grad_w, grad_b: 0.25413669101481196 , 0.006579057998705318 / Norm of weights: 3.319975888640672\n",
      "Iteration Number: 602 / Loss: 8.560055853470798 / Norm of grad_w, grad_b: 0.25274982855750583 , 0.0065788986551454254 / Norm of weights: 3.3199757433919794\n",
      "Iteration Number: 603 / Loss: 8.560058479812305 / Norm of grad_w, grad_b: 0.2513706121158717 , 0.006578739755975201 / Norm of weights: 3.3199756563707177\n",
      "Iteration Number: 604 / Loss: 8.560061319814878 / Norm of grad_w, grad_b: 0.24999899851537968 , 0.0065785812980582205 / Norm of weights: 3.319975626833153\n",
      "Iteration Number: 605 / Loss: 8.560064370817935 / Norm of grad_w, grad_b: 0.24863494483976503 , 0.006578423278280668 / Norm of weights: 3.319975654044028\n",
      "Iteration Number: 606 / Loss: 8.560067630190922 / Norm of grad_w, grad_b: 0.24727840842928053 , 0.006578265693551187 / Norm of weights: 3.3199757372764727\n",
      "Iteration Number: 607 / Loss: 8.560071095332987 / Norm of grad_w, grad_b: 0.24592934687897453 , 0.006578108540800817 / Norm of weights: 3.319975875811909\n",
      "Iteration Number: 608 / Loss: 8.560074763672667 / Norm of grad_w, grad_b: 0.2445877180369596 , 0.006577951816982611 / Norm of weights: 3.3199760689399653\n",
      "Iteration Number: 609 / Loss: 8.560078632667555 / Norm of grad_w, grad_b: 0.24325348000276675 , 0.006577795519071858 / Norm of weights: 3.319976315958383\n",
      "Iteration Number: 610 / Loss: 8.560082699804 / Norm of grad_w, grad_b: 0.2419265911256005 , 0.006577639644065601 / Norm of weights: 3.3199766161729305\n",
      "Iteration Number: 611 / Loss: 8.560086962596781 / Norm of grad_w, grad_b: 0.24060701000270357 , 0.006577484188982638 / Norm of weights: 3.3199769688973157\n",
      "Iteration Number: 612 / Loss: 8.560091418588812 / Norm of grad_w, grad_b: 0.23929469547772264 , 0.006577329150863338 / Norm of weights: 3.319977373453099\n",
      "Iteration Number: 613 / Loss: 8.560096065350825 / Norm of grad_w, grad_b: 0.2379896066390201 , 0.00657717452676956 / Norm of weights: 3.3199778291696083\n",
      "Iteration Number: 614 / Loss: 8.560100900481082 / Norm of grad_w, grad_b: 0.2366917028180838 , 0.006577020313784405 / Norm of weights: 3.319978335383855\n",
      "Iteration Number: 615 / Loss: 8.560105921605068 / Norm of grad_w, grad_b: 0.23540094358788863 , 0.006576866509012101 / Norm of weights: 3.319978891440447\n",
      "Iteration Number: 616 / Loss: 8.56011112637519 / Norm of grad_w, grad_b: 0.23411728876132326 , 0.006576713109577915 / Norm of weights: 3.3199794966915124\n",
      "Iteration Number: 617 / Loss: 8.5601165124705 / Norm of grad_w, grad_b: 0.23284069838957527 , 0.006576560112627942 / Norm of weights: 3.319980150496611\n",
      "Iteration Number: 618 / Loss: 8.560122077596395 / Norm of grad_w, grad_b: 0.23157113276056132 , 0.006576407515329002 / Norm of weights: 3.3199808522226566\n",
      "Iteration Number: 619 / Loss: 8.560127819484332 / Norm of grad_w, grad_b: 0.23030855239736414 , 0.006576255314868469 / Norm of weights: 3.3199816012438363\n",
      "Iteration Number: 620 / Loss: 8.560133735891549 / Norm of grad_w, grad_b: 0.22905291805672248 , 0.00657610350845416 / Norm of weights: 3.319982396941532\n",
      "Iteration Number: 621 / Loss: 8.560139824600784 / Norm of grad_w, grad_b: 0.2278041907273911 , 0.006575952093314158 / Norm of weights: 3.3199832387042396\n",
      "Iteration Number: 622 / Loss: 8.560146083419994 / Norm of grad_w, grad_b: 0.2265623316287675 , 0.0065758010666966925 / Norm of weights: 3.3199841259274945\n",
      "Iteration Number: 623 / Loss: 8.56015251018209 / Norm of grad_w, grad_b: 0.2253273022092298 , 0.006575650425870023 / Norm of weights: 3.319985058013792\n",
      "Iteration Number: 624 / Loss: 8.560159102744656 / Norm of grad_w, grad_b: 0.22409906414470743 , 0.006575500168122231 / Norm of weights: 3.3199860343725134\n",
      "Iteration Number: 625 / Loss: 8.560165858989686 / Norm of grad_w, grad_b: 0.22287757933722638 , 0.006575350290761183 / Norm of weights: 3.3199870544198493\n",
      "Iteration Number: 626 / Loss: 8.560172776823322 / Norm of grad_w, grad_b: 0.22166280991334097 , 0.006575200791114242 / Norm of weights: 3.319988117578726\n",
      "Iteration Number: 627 / Loss: 8.56017985417558 / Norm of grad_w, grad_b: 0.22045471822274912 , 0.006575051666528305 / Norm of weights: 3.3199892232787325\n",
      "Iteration Number: 628 / Loss: 8.560187089000106 / Norm of grad_w, grad_b: 0.21925326683678187 , 0.006574902914369607 / Norm of weights: 3.3199903709560457\n",
      "Iteration Number: 629 / Loss: 8.560194479273903 / Norm of grad_w, grad_b: 0.2180584185470138 , 0.006574754532023458 / Norm of weights: 3.3199915600533614\n",
      "Iteration Number: 630 / Loss: 8.560202022997096 / Norm of grad_w, grad_b: 0.21687013636378516 , 0.006574606516894306 / Norm of weights: 3.3199927900198207\n",
      "Iteration Number: 631 / Loss: 8.56020971819266 / Norm of grad_w, grad_b: 0.21568838351480216 , 0.006574458866405497 / Norm of weights: 3.31999406031094\n",
      "Iteration Number: 632 / Loss: 8.560217562906185 / Norm of grad_w, grad_b: 0.21451312344373488 , 0.006574311577999104 / Norm of weights: 3.3199953703885425\n",
      "Iteration Number: 633 / Loss: 8.560225555205628 / Norm of grad_w, grad_b: 0.21334431980880136 , 0.006574164649135921 / Norm of weights: 3.3199967197206863\n",
      "Iteration Number: 634 / Loss: 8.56023369318106 / Norm of grad_w, grad_b: 0.2121819364813938 , 0.006574018077295232 / Norm of weights: 3.3199981077816\n",
      "Iteration Number: 635 / Loss: 8.56024197494444 / Norm of grad_w, grad_b: 0.21102593754471746 , 0.0065738718599746915 / Norm of weights: 3.3199995340516124\n",
      "Iteration Number: 636 / Loss: 8.560250398629366 / Norm of grad_w, grad_b: 0.20987628729236146 , 0.00657372599469023 / Norm of weights: 3.320000998017087\n",
      "Iteration Number: 637 / Loss: 8.560258962390844 / Norm of grad_w, grad_b: 0.20873295022703808 , 0.006573580478975904 / Norm of weights: 3.320002499170355\n",
      "Iteration Number: 638 / Loss: 8.560267664405046 / Norm of grad_w, grad_b: 0.20759589105917348 , 0.0065734353103837385 / Norm of weights: 3.320004037009651\n",
      "Iteration Number: 639 / Loss: 8.560276502869103 / Norm of grad_w, grad_b: 0.2064650747055563 , 0.006573290486483694 / Norm of weights: 3.3200056110390475\n",
      "Iteration Number: 640 / Loss: 8.560285476000844 / Norm of grad_w, grad_b: 0.2053404662881097 , 0.00657314600486341 / Norm of weights: 3.3200072207683897\n",
      "Iteration Number: 641 / Loss: 8.56029458203859 / Norm of grad_w, grad_b: 0.20422203113245602 , 0.006573001863128199 / Norm of weights: 3.320008865713236\n",
      "Iteration Number: 642 / Loss: 8.56030381924093 / Norm of grad_w, grad_b: 0.20310973476670477 , 0.006572858058900811 / Norm of weights: 3.320010545394792\n",
      "Iteration Number: 643 / Loss: 8.560313185886496 / Norm of grad_w, grad_b: 0.2020035429201168 , 0.0065727145898214255 / Norm of weights: 3.320012259339849\n",
      "Iteration Number: 644 / Loss: 8.560322680273746 / Norm of grad_w, grad_b: 0.2009034215218333 , 0.006572571453547415 / Norm of weights: 3.320014007080724\n",
      "Iteration Number: 645 / Loss: 8.560332300720745 / Norm of grad_w, grad_b: 0.1998093366995918 , 0.006572428647753308 / Norm of weights: 3.3200157881551977\n",
      "Iteration Number: 646 / Loss: 8.560342045564948 / Norm of grad_w, grad_b: 0.19872125477846705 , 0.006572286170130625 / Norm of weights: 3.3200176021064554\n",
      "Iteration Number: 647 / Loss: 8.560351913163 / Norm of grad_w, grad_b: 0.19763914227962695 , 0.006572144018387729 / Norm of weights: 3.320019448483028\n",
      "Iteration Number: 648 / Loss: 8.560361901890522 / Norm of grad_w, grad_b: 0.19656296591906158 , 0.006572002190249811 / Norm of weights: 3.3200213268387304\n",
      "Iteration Number: 649 / Loss: 8.560372010141887 / Norm of grad_w, grad_b: 0.19549269260637853 , 0.006571860683458647 / Norm of weights: 3.320023236732608\n",
      "Iteration Number: 650 / Loss: 8.560382236330042 / Norm of grad_w, grad_b: 0.19442828944355595 , 0.006571719495772534 / Norm of weights: 3.3200251777288754\n",
      "Iteration Number: 651 / Loss: 8.560392578886283 / Norm of grad_w, grad_b: 0.19336972372371336 , 0.006571578624966182 / Norm of weights: 3.3200271493968616\n",
      "Iteration Number: 652 / Loss: 8.560403036260071 / Norm of grad_w, grad_b: 0.1923169629299443 , 0.006571438068830546 / Norm of weights: 3.320029151310953\n",
      "Iteration Number: 653 / Loss: 8.560413606918823 / Norm of grad_w, grad_b: 0.19126997473408244 , 0.006571297825172817 / Norm of weights: 3.3200311830505376\n",
      "Iteration Number: 654 / Loss: 8.560424289347715 / Norm of grad_w, grad_b: 0.190228726995497 , 0.006571157891816165 / Norm of weights: 3.3200332441999496\n",
      "Iteration Number: 655 / Loss: 8.560435082049489 / Norm of grad_w, grad_b: 0.1891931877599779 , 0.006571018266599722 / Norm of weights: 3.320035334348417\n",
      "Iteration Number: 656 / Loss: 8.56044598354427 / Norm of grad_w, grad_b: 0.18816332525847732 , 0.006570878947378426 / Norm of weights: 3.3200374530900048\n",
      "Iteration Number: 657 / Loss: 8.560456992369364 / Norm of grad_w, grad_b: 0.1871391079060164 , 0.006570739932022938 / Norm of weights: 3.320039600023564\n",
      "Iteration Number: 658 / Loss: 8.560468107079071 / Norm of grad_w, grad_b: 0.18612050430048313 , 0.006570601218419468 / Norm of weights: 3.320041774752678\n",
      "Iteration Number: 659 / Loss: 8.560479326244508 / Norm of grad_w, grad_b: 0.18510748322151904 , 0.006570462804469751 / Norm of weights: 3.3200439768856085\n",
      "Iteration Number: 660 / Loss: 8.560490648453408 / Norm of grad_w, grad_b: 0.18410001362934697 , 0.006570324688090788 / Norm of weights: 3.320046206035249\n",
      "Iteration Number: 661 / Loss: 8.560502072309962 / Norm of grad_w, grad_b: 0.18309806466366532 , 0.006570186867215002 / Norm of weights: 3.320048461819067\n",
      "Iteration Number: 662 / Loss: 8.560513596434612 / Norm of grad_w, grad_b: 0.1821016056425293 , 0.006570049339789764 / Norm of weights: 3.32005074385906\n",
      "Iteration Number: 663 / Loss: 8.560525219463898 / Norm of grad_w, grad_b: 0.18111060606120685 , 0.006569912103777612 / Norm of weights: 3.3200530517817\n",
      "Iteration Number: 664 / Loss: 8.560536940050252 / Norm of grad_w, grad_b: 0.18012503559111137 , 0.006569775157155987 / Norm of weights: 3.3200553852178887\n",
      "Iteration Number: 665 / Loss: 8.56054875686186 / Norm of grad_w, grad_b: 0.1791448640786834 , 0.006569638497917031 / Norm of weights: 3.3200577438029044\n",
      "Iteration Number: 666 / Loss: 8.56056066858245 / Norm of grad_w, grad_b: 0.17817006154428067 , 0.006569502124067747 / Norm of weights: 3.3200601271763572\n",
      "Iteration Number: 667 / Loss: 8.560572673911159 / Norm of grad_w, grad_b: 0.17720059818115327 , 0.0065693660336296365 / Norm of weights: 3.3200625349821378\n",
      "Iteration Number: 668 / Loss: 8.560584771562327 / Norm of grad_w, grad_b: 0.1762364443543143 , 0.006569230224638754 / Norm of weights: 3.320064966868374\n",
      "Iteration Number: 669 / Loss: 8.560596960265368 / Norm of grad_w, grad_b: 0.17527757059949858 , 0.0065690946951454275 / Norm of weights: 3.3200674224873805\n",
      "Iteration Number: 670 / Loss: 8.56060923876457 / Norm of grad_w, grad_b: 0.17432394762210693 , 0.006568959443214427 / Norm of weights: 3.320069901495613\n",
      "Iteration Number: 671 / Loss: 8.560621605818948 / Norm of grad_w, grad_b: 0.17337554629614452 , 0.006568824466924577 / Norm of weights: 3.320072403553624\n",
      "Iteration Number: 672 / Loss: 8.560634060202087 / Norm of grad_w, grad_b: 0.17243233766318586 , 0.006568689764368796 / Norm of weights: 3.3200749283260174\n",
      "Iteration Number: 673 / Loss: 8.560646600701968 / Norm of grad_w, grad_b: 0.17149429293133422 , 0.006568555333653999 / Norm of weights: 3.320077475481401\n",
      "Iteration Number: 674 / Loss: 8.560659226120823 / Norm of grad_w, grad_b: 0.1705613834741868 , 0.00656842117290096 / Norm of weights: 3.3200800446923444\n",
      "Iteration Number: 675 / Loss: 8.56067193527496 / Norm of grad_w, grad_b: 0.16963358082983807 , 0.006568287280244201 / Norm of weights: 3.3200826356353352\n",
      "Iteration Number: 676 / Loss: 8.560684726994634 / Norm of grad_w, grad_b: 0.16871085669983482 , 0.00656815365383191 / Norm of weights: 3.320085247990734\n",
      "Iteration Number: 677 / Loss: 8.560697600123868 / Norm of grad_w, grad_b: 0.16779318294820703 , 0.006568020291825854 / Norm of weights: 3.3200878814427326\n",
      "Iteration Number: 678 / Loss: 8.560710553520316 / Norm of grad_w, grad_b: 0.16688053160041058 , 0.006567887192401254 / Norm of weights: 3.320090535679311\n",
      "Iteration Number: 679 / Loss: 8.560723586055108 / Norm of grad_w, grad_b: 0.1659728748424149 , 0.006567754353746686 / Norm of weights: 3.320093210392196\n",
      "Iteration Number: 680 / Loss: 8.560736696612702 / Norm of grad_w, grad_b: 0.16507018501964102 , 0.006567621774064008 / Norm of weights: 3.3200959052768177\n",
      "Iteration Number: 681 / Loss: 8.560749884090738 / Norm of grad_w, grad_b: 0.1641724346360226 , 0.006567489451568237 / Norm of weights: 3.3200986200322715\n",
      "Iteration Number: 682 / Loss: 8.56076314739989 / Norm of grad_w, grad_b: 0.16327959635303213 , 0.006567357384487454 / Norm of weights: 3.3201013543612734\n",
      "Iteration Number: 683 / Loss: 8.560776485463725 / Norm of grad_w, grad_b: 0.16239164298872488 , 0.006567225571062721 / Norm of weights: 3.320104107970124\n",
      "Iteration Number: 684 / Loss: 8.56078989721856 / Norm of grad_w, grad_b: 0.16150854751675947 , 0.006567094009547958 / Norm of weights: 3.320106880568665\n",
      "Iteration Number: 685 / Loss: 8.560803381613319 / Norm of grad_w, grad_b: 0.1606302830654447 , 0.006566962698209957 / Norm of weights: 3.3201096718702416\n",
      "Iteration Number: 686 / Loss: 8.560816937609395 / Norm of grad_w, grad_b: 0.15975682291682125 , 0.006566831635328053 / Norm of weights: 3.320112481591663\n",
      "Iteration Number: 687 / Loss: 8.560830564180511 / Norm of grad_w, grad_b: 0.1588881405057114 , 0.0065667008191943295 / Norm of weights: 3.3201153094531644\n",
      "Iteration Number: 688 / Loss: 8.560844260312589 / Norm of grad_w, grad_b: 0.15802420941878076 , 0.006566570248113216 / Norm of weights: 3.320118155178368\n",
      "Iteration Number: 689 / Loss: 8.56085802500361 / Norm of grad_w, grad_b: 0.15716500339362685 , 0.006566439920401681 / Norm of weights: 3.3201210184942447\n",
      "Iteration Number: 690 / Loss: 8.560871857263475 / Norm of grad_w, grad_b: 0.15631049631784213 , 0.006566309834388972 / Norm of weights: 3.3201238991310795\n",
      "Iteration Number: 691 / Loss: 8.56088575611389 / Norm of grad_w, grad_b: 0.15546066222813695 , 0.006566179988416508 / Norm of weights: 3.3201267968224313\n",
      "Iteration Number: 692 / Loss: 8.56089972058822 / Norm of grad_w, grad_b: 0.15461547530935804 , 0.006566050380837925 / Norm of weights: 3.320129711305097\n",
      "Iteration Number: 693 / Loss: 8.560913749731357 / Norm of grad_w, grad_b: 0.15377490989372178 , 0.006565921010018834 / Norm of weights: 3.320132642319079\n",
      "Iteration Number: 694 / Loss: 8.560927842599616 / Norm of grad_w, grad_b: 0.15293894045978862 , 0.0065657918743368664 / Norm of weights: 3.320135589607543\n",
      "Iteration Number: 695 / Loss: 8.560941998260581 / Norm of grad_w, grad_b: 0.15210754163161794 , 0.00656566297218144 / Norm of weights: 3.320138552916787\n",
      "Iteration Number: 696 / Loss: 8.560956215792988 / Norm of grad_w, grad_b: 0.15128068817795368 , 0.006565534301953854 / Norm of weights: 3.320141531996206\n",
      "Iteration Number: 697 / Loss: 8.560970494286614 / Norm of grad_w, grad_b: 0.15045835501123928 , 0.006565405862067015 / Norm of weights: 3.3201445265982574\n",
      "Iteration Number: 698 / Loss: 8.560984832842145 / Norm of grad_w, grad_b: 0.14964051718686297 , 0.00656527765094545 / Norm of weights: 3.3201475364784234\n",
      "Iteration Number: 699 / Loss: 8.560999230571044 / Norm of grad_w, grad_b: 0.14882714990217313 , 0.006565149667025251 / Norm of weights: 3.3201505613951823\n",
      "Iteration Number: 700 / Loss: 8.561013686595453 / Norm of grad_w, grad_b: 0.1480182284957153 , 0.006565021908753875 / Norm of weights: 3.3201536011099706\n",
      "Iteration Number: 701 / Loss: 8.561028200048058 / Norm of grad_w, grad_b: 0.1472137284463879 , 0.006564894374590191 / Norm of weights: 3.320156655387153\n",
      "Iteration Number: 702 / Loss: 8.56104277007198 / Norm of grad_w, grad_b: 0.14641362537249064 , 0.006564767063004264 / Norm of weights: 3.320159723993986\n",
      "Iteration Number: 703 / Loss: 8.561057395820653 / Norm of grad_w, grad_b: 0.14561789503101422 , 0.006564639972477353 / Norm of weights: 3.32016280670059\n",
      "Iteration Number: 704 / Loss: 8.561072076457714 / Norm of grad_w, grad_b: 0.14482651331671967 , 0.006564513101501885 / Norm of weights: 3.3201659032799142\n",
      "Iteration Number: 705 / Loss: 8.561086811156892 / Norm of grad_w, grad_b: 0.1440394562613446 , 0.006564386448581203 / Norm of weights: 3.320169013507704\n",
      "Iteration Number: 706 / Loss: 8.561101599101885 / Norm of grad_w, grad_b: 0.14325670003277652 , 0.006564260012229622 / Norm of weights: 3.320172137162473\n",
      "Iteration Number: 707 / Loss: 8.561116439486256 / Norm of grad_w, grad_b: 0.14247822093423645 , 0.0065641337909723456 / Norm of weights: 3.3201752740254675\n",
      "Iteration Number: 708 / Loss: 8.561131331513327 / Norm of grad_w, grad_b: 0.14170399540347695 , 0.0065640077833452335 / Norm of weights: 3.3201784238806407\n",
      "Iteration Number: 709 / Loss: 8.561146274396066 / Norm of grad_w, grad_b: 0.1409340000119303 , 0.006563881987894951 / Norm of weights: 3.3201815865146176\n",
      "Iteration Number: 710 / Loss: 8.561161267356969 / Norm of grad_w, grad_b: 0.14016821146398484 , 0.006563756403178674 / Norm of weights: 3.320184761716669\n",
      "Iteration Number: 711 / Loss: 8.561176309627978 / Norm of grad_w, grad_b: 0.13940660659612888 , 0.006563631027764207 / Norm of weights: 3.320187949278678\n",
      "Iteration Number: 712 / Loss: 8.561191400450353 / Norm of grad_w, grad_b: 0.1386491623761855 , 0.006563505860229719 / Norm of weights: 3.3201911489951135\n",
      "Iteration Number: 713 / Loss: 8.561206539074577 / Norm of grad_w, grad_b: 0.13789585590252482 , 0.006563380899163757 / Norm of weights: 3.320194360662999\n",
      "Iteration Number: 714 / Loss: 8.561221724760255 / Norm of grad_w, grad_b: 0.13714666440328804 , 0.006563256143165195 / Norm of weights: 3.3201975840818854\n",
      "Iteration Number: 715 / Loss: 8.561236956776007 / Norm of grad_w, grad_b: 0.13640156523560018 , 0.006563131590843107 / Norm of weights: 3.3202008190538215\n",
      "Iteration Number: 716 / Loss: 8.561252234399369 / Norm of grad_w, grad_b: 0.13566053588481133 , 0.006563007240816738 / Norm of weights: 3.3202040653833262\n",
      "Iteration Number: 717 / Loss: 8.5612675569167 / Norm of grad_w, grad_b: 0.13492355396374506 , 0.006562883091715326 / Norm of weights: 3.32020732287736\n",
      "Iteration Number: 718 / Loss: 8.561282923623066 / Norm of grad_w, grad_b: 0.1341905972119412 , 0.006562759142178138 / Norm of weights: 3.320210591345298\n",
      "Iteration Number: 719 / Loss: 8.561298333822164 / Norm of grad_w, grad_b: 0.13346164349484474 , 0.006562635390854346 / Norm of weights: 3.320213870598904\n",
      "Iteration Number: 720 / Loss: 8.561313786826211 / Norm of grad_w, grad_b: 0.13273667080314405 , 0.006562511836402992 / Norm of weights: 3.3202171604523\n",
      "Iteration Number: 721 / Loss: 8.561329281955855 / Norm of grad_w, grad_b: 0.13201565725197845 , 0.006562388477492828 / Norm of weights: 3.3202204607219414\n",
      "Iteration Number: 722 / Loss: 8.561344818540068 / Norm of grad_w, grad_b: 0.13129858108019346 , 0.006562265312802324 / Norm of weights: 3.320223771226593\n",
      "Iteration Number: 723 / Loss: 8.561360395916077 / Norm of grad_w, grad_b: 0.13058542064962494 , 0.006562142341019577 / Norm of weights: 3.3202270917872987\n",
      "Iteration Number: 724 / Loss: 8.561376013429248 / Norm of grad_w, grad_b: 0.12987615444438308 , 0.006562019560842225 / Norm of weights: 3.3202304222273575\n",
      "Iteration Number: 725 / Loss: 8.561391670433006 / Norm of grad_w, grad_b: 0.1291707610700947 , 0.00656189697097735 / Norm of weights: 3.320233762372299\n",
      "Iteration Number: 726 / Loss: 8.561407366288742 / Norm of grad_w, grad_b: 0.12846921925322238 , 0.006561774570141512 / Norm of weights: 3.3202371120498553\n",
      "Iteration Number: 727 / Loss: 8.561423100365722 / Norm of grad_w, grad_b: 0.12777150784030372 , 0.006561652357060507 / Norm of weights: 3.3202404710899405\n",
      "Iteration Number: 728 / Loss: 8.561438872041009 / Norm of grad_w, grad_b: 0.1270776057972877 , 0.006561530330469437 / Norm of weights: 3.32024383932462\n",
      "Iteration Number: 729 / Loss: 8.56145468069935 / Norm of grad_w, grad_b: 0.1263874922088074 , 0.006561408489112637 / Norm of weights: 3.3202472165880916\n",
      "Iteration Number: 730 / Loss: 8.561470525733121 / Norm of grad_w, grad_b: 0.12570114627746715 , 0.006561286831743491 / Norm of weights: 3.320250602716657\n",
      "Iteration Number: 731 / Loss: 8.561486406542219 / Norm of grad_w, grad_b: 0.12501854732319287 , 0.0065611653571244575 / Norm of weights: 3.320253997548702\n",
      "Iteration Number: 732 / Loss: 8.561502322533984 / Norm of grad_w, grad_b: 0.12433967478247399 , 0.0065610440640270825 / Norm of weights: 3.320257400924669\n",
      "Iteration Number: 733 / Loss: 8.56151827312312 / Norm of grad_w, grad_b: 0.12366450820774506 , 0.006560922951231654 / Norm of weights: 3.3202608126870343\n",
      "Iteration Number: 734 / Loss: 8.561534257731605 / Norm of grad_w, grad_b: 0.12299302726665426 , 0.00656080201752745 / Norm of weights: 3.3202642326802887\n",
      "Iteration Number: 735 / Loss: 8.561550275788612 / Norm of grad_w, grad_b: 0.12232521174140125 , 0.006560681261712464 / Norm of weights: 3.3202676607509085\n",
      "Iteration Number: 736 / Loss: 8.561566326730423 / Norm of grad_w, grad_b: 0.12166104152807714 , 0.006560560682593443 / Norm of weights: 3.3202710967473372\n",
      "Iteration Number: 737 / Loss: 8.561582410000362 / Norm of grad_w, grad_b: 0.12100049663595878 , 0.006560440278985762 / Norm of weights: 3.3202745405199625\n",
      "Iteration Number: 738 / Loss: 8.5615985250487 / Norm of grad_w, grad_b: 0.12034355718691739 , 0.006560320049713393 / Norm of weights: 3.320277991921092\n",
      "Iteration Number: 739 / Loss: 8.561614671332581 / Norm of grad_w, grad_b: 0.11969020341465222 , 0.006560199993608881 / Norm of weights: 3.3202814508049343\n",
      "Iteration Number: 740 / Loss: 8.561630848315959 / Norm of grad_w, grad_b: 0.11904041566414528 , 0.0065600801095131046 / Norm of weights: 3.3202849170275734\n",
      "Iteration Number: 741 / Loss: 8.56164705546949 / Norm of grad_w, grad_b: 0.11839417439092484 , 0.006559960396275461 / Norm of weights: 3.3202883904469513\n",
      "Iteration Number: 742 / Loss: 8.561663292270486 / Norm of grad_w, grad_b: 0.11775146016045905 , 0.00655984085275363 / Norm of weights: 3.320291870922844\n",
      "Iteration Number: 743 / Loss: 8.561679558202826 / Norm of grad_w, grad_b: 0.11711225364750315 , 0.0065597214778135675 / Norm of weights: 3.320295358316843\n",
      "Iteration Number: 744 / Loss: 8.561695852756888 / Norm of grad_w, grad_b: 0.11647653563545568 , 0.006559602270329425 / Norm of weights: 3.3202988524923307\n",
      "Iteration Number: 745 / Loss: 8.561712175429463 / Norm of grad_w, grad_b: 0.11584428701574165 , 0.006559483229183517 / Norm of weights: 3.320302353314463\n",
      "Iteration Number: 746 / Loss: 8.561728525723693 / Norm of grad_w, grad_b: 0.11521548878716621 , 0.006559364353266212 / Norm of weights: 3.3203058606501483\n",
      "Iteration Number: 747 / Loss: 8.561744903148998 / Norm of grad_w, grad_b: 0.11459012205527376 , 0.00655924564147592 / Norm of weights: 3.320309374368027\n",
      "Iteration Number: 748 / Loss: 8.561761307221007 / Norm of grad_w, grad_b: 0.11396816803176785 , 0.006559127092719025 / Norm of weights: 3.3203128943384526\n",
      "Iteration Number: 749 / Loss: 8.561777737461473 / Norm of grad_w, grad_b: 0.11334960803383566 , 0.00655900870590979 / Norm of weights: 3.3203164204334685\n",
      "Iteration Number: 750 / Loss: 8.561794193398221 / Norm of grad_w, grad_b: 0.11273442348358459 , 0.006558890479970315 / Norm of weights: 3.320319952526794\n",
      "Iteration Number: 751 / Loss: 8.56181067456507 / Norm of grad_w, grad_b: 0.11212259590740768 , 0.006558772413830517 / Norm of weights: 3.3203234904938013\n",
      "Iteration Number: 752 / Loss: 8.561827180501773 / Norm of grad_w, grad_b: 0.11151410693535788 , 0.006558654506428026 / Norm of weights: 3.320327034211497\n",
      "Iteration Number: 753 / Loss: 8.56184371075393 / Norm of grad_w, grad_b: 0.11090893830060501 , 0.006558536756708083 / Norm of weights: 3.3203305835585035\n",
      "Iteration Number: 754 / Loss: 8.561860264872944 / Norm of grad_w, grad_b: 0.11030707183876376 , 0.006558419163623612 / Norm of weights: 3.3203341384150424\n",
      "Iteration Number: 755 / Loss: 8.561876842415948 / Norm of grad_w, grad_b: 0.10970848948734162 , 0.006558301726135062 / Norm of weights: 3.3203376986629114\n",
      "Iteration Number: 756 / Loss: 8.561893442945728 / Norm of grad_w, grad_b: 0.10911317328513939 , 0.006558184443210347 / Norm of weights: 3.3203412641854713\n",
      "Iteration Number: 757 / Loss: 8.561910066030672 / Norm of grad_w, grad_b: 0.10852110537166662 , 0.006558067313824864 / Norm of weights: 3.3203448348676243\n",
      "Iteration Number: 758 / Loss: 8.561926711244702 / Norm of grad_w, grad_b: 0.10793226798653653 , 0.006557950336961334 / Norm of weights: 3.3203484105957983\n",
      "Iteration Number: 759 / Loss: 8.561943378167214 / Norm of grad_w, grad_b: 0.10734664346889916 , 0.0065578335116098785 / Norm of weights: 3.3203519912579273\n",
      "Iteration Number: 760 / Loss: 8.561960066383005 / Norm of grad_w, grad_b: 0.10676421425688816 , 0.00655771683676781 / Norm of weights: 3.3203555767434354\n",
      "Iteration Number: 761 / Loss: 8.561976775482218 / Norm of grad_w, grad_b: 0.10618496288701101 , 0.006557600311439709 / Norm of weights: 3.320359166943221\n",
      "Iteration Number: 762 / Loss: 8.561993505060293 / Norm of grad_w, grad_b: 0.10560887199358183 , 0.006557483934637297 / Norm of weights: 3.3203627617496347\n",
      "Iteration Number: 763 / Loss: 8.562010254717878 / Norm of grad_w, grad_b: 0.10503592430818755 , 0.006557367705379395 / Norm of weights: 3.320366361056469\n",
      "Iteration Number: 764 / Loss: 8.562027024060798 / Norm of grad_w, grad_b: 0.104466102659103 , 0.006557251622691881 / Norm of weights: 3.320369964758936\n",
      "Iteration Number: 765 / Loss: 8.56204381269998 / Norm of grad_w, grad_b: 0.103899389970708 , 0.006557135685607632 / Norm of weights: 3.3203735727536543\n",
      "Iteration Number: 766 / Loss: 8.56206062025139 / Norm of grad_w, grad_b: 0.10333576926298989 , 0.0065570198931665085 / Norm of weights: 3.320377184938631\n",
      "Iteration Number: 767 / Loss: 8.562077446336 / Norm of grad_w, grad_b: 0.10277522365094822 , 0.0065569042444151675 / Norm of weights: 3.3203808012132474\n",
      "Iteration Number: 768 / Loss: 8.562094290579696 / Norm of grad_w, grad_b: 0.1022177363440482 , 0.006556788738407234 / Norm of weights: 3.3203844214782396\n",
      "Iteration Number: 769 / Loss: 8.562111152613252 / Norm of grad_w, grad_b: 0.10166329064570864 , 0.006556673374203014 / Norm of weights: 3.3203880456356867\n",
      "Iteration Number: 770 / Loss: 8.562128032072252 / Norm of grad_w, grad_b: 0.10111186995270562 , 0.006556558150869638 / Norm of weights: 3.3203916735889925\n",
      "Iteration Number: 771 / Loss: 8.56214492859705 / Norm of grad_w, grad_b: 0.10056345775469226 , 0.006556443067480835 / Norm of weights: 3.3203953052428705\n",
      "Iteration Number: 772 / Loss: 8.562161841832705 / Norm of grad_w, grad_b: 0.10001803763362961 , 0.006556328123117022 / Norm of weights: 3.320398940503331\n",
      "Iteration Number: 773 / Loss: 8.562178771428936 / Norm of grad_w, grad_b: 0.09947559326326393 , 0.006556213316865246 / Norm of weights: 3.320402579277661\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "lambda_ = 1.0\n",
    "\n",
    "model = fit(xtrain_normal, ytrain, learning_rate, lambda_, 10000, verbose=1) #keep the verbose on here for your submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.936923076923077\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: \", accuracy(xtrain_normal, ytrain, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 5 0.8391608391608392\n",
      "0.01 2 0.8551448551448552\n",
      "0.01 1 0.8571428571428571\n",
      "0.01 0.1 0.8861138861138861\n",
      "0.01 0.01 0.8681318681318682\n",
      "0.001 5 0.9200799200799201\n",
      "0.001 2 0.9180819180819181\n",
      "0.001 1 0.916083916083916\n",
      "0.001 0.1 0.9150849150849151\n",
      "0.001 0.01 0.9150849150849151\n",
      "0.0001 5 0.9230769230769231\n",
      "0.0001 2 0.919080919080919\n",
      "0.0001 1 0.9140859140859141\n",
      "0.0001 0.1 0.9170829170829171\n",
      "0.0001 0.01 0.9150849150849151\n",
      "1e-05 5 0.9090909090909091\n",
      "1e-05 2 0.913086913086913\n",
      "1e-05 1 0.9230769230769231\n",
      "1e-05 0.1 0.9170829170829171\n",
      "1e-05 0.01 0.9150849150849151\n"
     ]
    }
   ],
   "source": [
    "#grid search for finding the best hyperparams and model\n",
    "\n",
    "best_model = None\n",
    "best_val = -1\n",
    "for lr in [0.01, 0.001, 0.0001, 0.00001]:\n",
    "    for la in [5, 2, 1, 0.1, 0.01]:\n",
    "        model = fit(xtrain_normal, ytrain, lr, la, 10000, verbose=0)\n",
    "        val_acc = accuracy(xval_normal, yval, model)\n",
    "        print(lr, la, val_acc)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_model = model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.945\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: \", accuracy(xtest_normal, ytest, best_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
