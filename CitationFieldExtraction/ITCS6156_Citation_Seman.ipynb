{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZECBpTKulskT"
   },
   "source": [
    "Name:Matthew Seman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AmpvFq9NahJ3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ov7317d8axoS"
   },
   "outputs": [],
   "source": [
    "CORA='./CORA/'\n",
    "\n",
    "with open(CORA + 'vocab-lower.pickle', 'rb') as f:\n",
    "    #vocab dic is list of all words in the dictionery\n",
    "    vocab_dic = list(map(lambda x: x.decode('utf-8'), pickle.load(f, encoding='utf-8')))\n",
    "\n",
    "\n",
    "with open(CORA + 'vocab_labels.pickle', 'rb') as f:\n",
    "    #The list of possible labels \n",
    "    label_dic = list(map(lambda x: x.decode('utf-8'), pickle.load(f, encoding='bytes')))\n",
    "    \n",
    "with open(CORA + 'trained_embedding.pickle', 'rb') as f:\n",
    "    #The trained embedding on Wikipedia using GLOVE algorithm \n",
    "    #Each column is the embedding of a word (with respect to the vocab dictionary) \n",
    "    embedding = pickle.load(f, encoding='bytes')\n",
    "    vocabulary_size = 20608\n",
    "    embedding_size = 512\n",
    "\n",
    "\n",
    "with open(CORA + 'xdata-lower.pickle', 'rb') as f:\n",
    "    #Each row is a citation. Each cell is the token id (with respect to the vocab dictionary) within the citation.\n",
    "    xdata = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'Y_train.pickle', 'rb') as f:\n",
    "    ydata = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'xval-lower.pickle', 'rb') as f:\n",
    "    xval = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'Y_dev.pickle', 'rb') as f:\n",
    "    yval = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'xtest-lower.pickle', 'rb') as f:\n",
    "    xtest = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'Y_test.pickle', 'rb') as f:\n",
    "    ytest = pickle.load(f, encoding='bytes')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTXl5Ob2bQtC",
    "outputId": "80dc23a6-7803-4e40-8fd3-bc61761ba976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 18049\n",
      "no formal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"vocab size:\", len(vocab_dic))\n",
    "print(vocab_dic[100], vocab_dic[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9Z09S8ClbVvw"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "  norm_v1 = np.linalg.norm(v1)\n",
    "  norm_v2 = np.linalg.norm(v2)\n",
    "  return np.dot(v1,v2)/(norm_v1*norm_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KGKg5ll-cU4O"
   },
   "outputs": [],
   "source": [
    "v1_id = vocab_dic.index('positive')\n",
    "v2_id = vocab_dic.index('constructive')\n",
    "v3_id = vocab_dic.index('conference')\n",
    "\n",
    "v1_vec = embedding[v1_id]\n",
    "v2_vec = embedding[v2_id]\n",
    "v3_vec = embedding[v3_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmLEhiIxc7FO",
    "outputId": "ea4aed65-bd99-4176-b763-70fde795ef75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4082968846811103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1_vec,v2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rx5u_An5eMTp",
    "outputId": "e8634789-db63-40c1-dc2e-2877f9d7fdae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22629842691063806"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1_vec, v3_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnxWHKR0ePbV",
    "outputId": "a2887935-e503-4915-e2d3-9e43fd1583aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   16     1  4426     2    33     1  2014     2     4    38     1  8853\n",
      "     3 14152     1   189     5  1901    37    59   839   128     4   319\n",
      "   155     1   102   282    62     2    46     7    46     8     2   297\n",
      "     1     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#construct the sentence\n",
    "print(xdata[1])\n",
    "#each cell is the token id and 0 means padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KPfk5QVjebKo"
   },
   "outputs": [],
   "source": [
    "def get_sentence(x):\n",
    "  x_nopad = list(filter(lambda i: True if i > 0 else False, x))\n",
    "  return ' '.join([vocab_dic[id] for id in x_nopad])\n",
    "\n",
    "def get_label(y):\n",
    "  y_nopad = list(filter(lambda i: True if i > 0 else False, y))\n",
    "  return ' '.join([label_dic[id] for id in y_nopad])\n",
    "\n",
    "def print_sentence_label_pair(x, y):\n",
    "  x_nopad = list(filter(lambda i: True if i > 0 else False, x))\n",
    "  y_nopad = list(filter(lambda i: True if i > 0 else False, y))\n",
    "  for i in range(len(x_nopad)):\n",
    "    print(label_dic[y_nopad[i]],'\\t\\t' ,vocab_dic[x_nopad[i]],)\n",
    "\n",
    "def print_sentence_pred_label(x, y, yt):\n",
    "  x_nopad = list(filter(lambda i: True if i > 0 else False, x))\n",
    "  for i in range(len(x_nopad)):\n",
    "    print(label_dic[yt[i]], label_dic[y[i]],'\\t\\t'  ,vocab_dic[x_nopad[i]],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sl8DO5jJgB68",
    "outputId": "7f4503b6-bca9-478a-c9f4-204e6d51a69e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author \t\t ramadge\n",
      "author \t\t ,\n",
      "author \t\t p\n",
      "author \t\t .\n",
      "author \t\t ,\n",
      "author \t\t &\n",
      "author \t\t wonham\n",
      "author \t\t ,\n",
      "author \t\t w\n",
      "author \t\t .\n",
      "date \t\t (\n",
      "date \t\t 1989\n",
      "date \t\t )\n",
      "date \t\t .\n",
      "title \t\t the\n",
      "title \t\t control\n",
      "title \t\t of\n",
      "title \t\t discrete\n",
      "title \t\t event\n",
      "title \t\t systems\n",
      "title \t\t .\n",
      "booktitle \t\t proceedings\n",
      "booktitle \t\t of\n",
      "booktitle \t\t the\n",
      "booktitle \t\t ieee\n",
      "booktitle \t\t ,\n",
      "volume \t\t 77\n",
      "volume \t\t (\n",
      "volume \t\t 1\n",
      "volume \t\t )\n",
      "volume \t\t ,\n",
      "pages \t\t 81\n",
      "pages \t\t -\n",
      "pages \t\t 98\n",
      "pages \t\t .\n"
     ]
    }
   ],
   "source": [
    "print_sentence_label_pair(xdata[10], ydata[10])\n",
    "# print(get_sentence(xdata[10]))\n",
    "# print(get_label(ydata[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nbU7C7R1RMb4"
   },
   "outputs": [],
   "source": [
    "#calucate the hamming loss between gold data and predication\n",
    "def token_level_loss(cpred, ctrue):\n",
    "  try:\n",
    "    label_num = np.shape(cpred)[1]\n",
    "    pred = cpred.astype(np.int)\n",
    "    true = ctrue.astype(np.int)\n",
    "    n1 = len(pred)\n",
    "    n2 = len(true)\n",
    "    assert n1 == n2\n",
    "    hloss = 0.0\n",
    "    exact_acc = 0.0\n",
    "    for i in range(n1):\n",
    "      sample_loss = 0.0\n",
    "      exact = 0.0\n",
    "      xp = pred[i]\n",
    "      xt = true[i]\n",
    "      cnp = 0\n",
    "      for j in range(label_num):\n",
    "        if (xt[j] == 0):\n",
    "          continue\n",
    "        cnp+=1\n",
    "        if xp[j] != xt[j]:\n",
    "          sample_loss += 1.0\n",
    "        else:\n",
    "          exact +=1\n",
    "      sample_loss = sample_loss / cnp\n",
    "      exact = exact / cnp\n",
    "      hloss += sample_loss\n",
    "      exact_acc += exact\n",
    "    return (hloss / n1, exact_acc / n1)\n",
    "  except Warning:\n",
    "    return (0.0,0.0);\n",
    "\n",
    "def perf(ytr_pred, yval_pred, yts_pred, ydata, yval, ytest):\n",
    "    global best_val\n",
    "    global test_val\n",
    "\n",
    "    hm_ts, ex_ts = token_level_loss(yts_pred, ytest)\n",
    "    hm_tr, ex_tr = token_level_loss(ytr_pred, ydata)\n",
    "    hm_val, ex_val = token_level_loss(yval_pred, yval)\n",
    "    if ex_val > best_val:\n",
    "        best_val = ex_val\n",
    "        test_val = ex_ts\n",
    "    return (\"Train: %0.3f Val: %0.3f Test: %0.3f -- Best Val: %0.3f Test: %0.3f\" % (ex_tr, ex_val, ex_ts, best_val, test_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JSRNVhcugZZv"
   },
   "outputs": [],
   "source": [
    "output_size = len(label_dic)\n",
    "max_length_output = 118\n",
    "class CitationNetwork(tf.keras.Model):\n",
    "  def __init__(self, network_type = 'SimpleLSTM'):\n",
    "      super(CitationNetwork, self).__init__()\n",
    "\n",
    "      self.optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "      self.embedding = layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_size)\n",
    "      #Encoder-Decoder (3 - outputs)\n",
    "      self.encoder = layers.LSTM(128, return_sequences=True, return_state=True)\n",
    "      self.decoder = layers.LSTM(64, return_sequences=True, return_state=True)\n",
    "      \n",
    "      #Recurrent (3 - outputs)\n",
    "      self.rnn = layers.LSTM(128, return_sequences=True, return_state=True)\n",
    "        \n",
    "      #Bidirection (5 - outputs)\n",
    "      self.bdnn1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, return_state=True, dropout=0.0, recurrent_dropout=0.0))\n",
    "      self.bdnn2 = layers.Bidirectional(layers.LSTM(64, return_sequences=True, return_state=True, dropout=0.0, recurrent_dropout=0.0))\n",
    "      self.bdnn3 = layers.Bidirectional(layers.LSTM(32, return_sequences=True, return_state=True, dropout=0.0, recurrent_dropout=0.0))\n",
    "      self.bdnn4 = layers.Bidirectional(layers.LSTM(16, return_sequences=True, return_state=True, dropout=0.0, recurrent_dropout=0.0))\n",
    "        \n",
    "      self.fc = layers.Dense(output_size)\n",
    "  \n",
    " \n",
    "  def call(self, x, predict=True):\n",
    "      emb = self.embedding(x)\n",
    "      output, state_h, state_c, _ , _ = self.bdnn1(emb)\n",
    "      output, state_h, state_c, _ , _ = self.bdnn2(output)\n",
    "      output = self.fc(output) \n",
    "      if predict:\n",
    "        return tf.math.argmax(output, axis=-1)\n",
    "      return output\n",
    "  \n",
    "  def get_loss(self, ylogits, yt):\n",
    "    ylabels = tf.one_hot(ybatch, output_size)\n",
    "    cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=ylogits, labels=ylabels);\n",
    "    return tf.reduce_sum(cross_ent)\n",
    "\n",
    "@tf.function\n",
    "def train_step(xbatch, ybatch):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = cite(xbatch, False)\n",
    "        loss = cite.get_loss(prediction, ybatch)\n",
    "        \n",
    "    gradients = tape.gradient(loss, cite.trainable_variables)\n",
    "    cite.optimizer.apply_gradients(zip(gradients, cite.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "cite = CitationNetwork();\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_2HngfD8jrBE",
    "outputId": "aadc49dd-c132-46e5-b145-eadea2c78998"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\AppData\\Local\\Temp/ipykernel_8552/2381813504.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pred = cpred.astype(np.int)\n",
      "C:\\Users\\Matthew\\AppData\\Local\\Temp/ipykernel_8552/2381813504.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  true = ctrue.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 76394.55 Train: 0.000 Val: 0.000 Test: 0.000 -- Best Val: 0.000 Test: 0.000\n",
      "1 36514.38 Train: 0.000 Val: 0.000 Test: 0.000 -- Best Val: 0.000 Test: 0.000\n",
      "2 32895.395 Train: 0.004 Val: 0.004 Test: 0.003 -- Best Val: 0.004 Test: 0.003\n",
      "3 30839.45 Train: 0.088 Val: 0.076 Test: 0.067 -- Best Val: 0.076 Test: 0.067\n",
      "4 27213.47 Train: 0.349 Val: 0.369 Test: 0.388 -- Best Val: 0.369 Test: 0.388\n",
      "5 23082.426 Train: 0.357 Val: 0.372 Test: 0.400 -- Best Val: 0.372 Test: 0.400\n",
      "6 20869.266 Train: 0.357 Val: 0.374 Test: 0.398 -- Best Val: 0.374 Test: 0.398\n",
      "7 20148.914 Train: 0.358 Val: 0.371 Test: 0.395 -- Best Val: 0.374 Test: 0.398\n",
      "8 19369.094 Train: 0.361 Val: 0.374 Test: 0.397 -- Best Val: 0.374 Test: 0.397\n",
      "9 19036.166 Train: 0.364 Val: 0.376 Test: 0.402 -- Best Val: 0.376 Test: 0.402\n",
      "10 18704.803 Train: 0.366 Val: 0.377 Test: 0.399 -- Best Val: 0.377 Test: 0.399\n",
      "11 18505.678 Train: 0.361 Val: 0.370 Test: 0.397 -- Best Val: 0.377 Test: 0.399\n",
      "12 18313.363 Train: 0.359 Val: 0.371 Test: 0.395 -- Best Val: 0.377 Test: 0.399\n",
      "13 18233.02 Train: 0.360 Val: 0.371 Test: 0.394 -- Best Val: 0.377 Test: 0.399\n",
      "14 18056.91 Train: 0.362 Val: 0.370 Test: 0.397 -- Best Val: 0.377 Test: 0.399\n",
      "15 17854.953 Train: 0.362 Val: 0.371 Test: 0.396 -- Best Val: 0.377 Test: 0.399\n",
      "16 17767.262 Train: 0.364 Val: 0.375 Test: 0.400 -- Best Val: 0.377 Test: 0.399\n",
      "17 17717.271 Train: 0.362 Val: 0.371 Test: 0.395 -- Best Val: 0.377 Test: 0.399\n",
      "18 17655.879 Train: 0.365 Val: 0.373 Test: 0.399 -- Best Val: 0.377 Test: 0.399\n",
      "19 17515.996 Train: 0.361 Val: 0.371 Test: 0.395 -- Best Val: 0.377 Test: 0.399\n",
      "20 17367.182 Train: 0.366 Val: 0.376 Test: 0.400 -- Best Val: 0.377 Test: 0.399\n",
      "21 17357.63 Train: 0.367 Val: 0.377 Test: 0.401 -- Best Val: 0.377 Test: 0.399\n",
      "22 17222.652 Train: 0.366 Val: 0.375 Test: 0.399 -- Best Val: 0.377 Test: 0.399\n",
      "23 17148.938 Train: 0.368 Val: 0.377 Test: 0.399 -- Best Val: 0.377 Test: 0.399\n",
      "24 17128.45 Train: 0.367 Val: 0.376 Test: 0.400 -- Best Val: 0.377 Test: 0.399\n",
      "25 17065.965 Train: 0.369 Val: 0.376 Test: 0.401 -- Best Val: 0.377 Test: 0.399\n",
      "26 17003.604 Train: 0.370 Val: 0.379 Test: 0.404 -- Best Val: 0.379 Test: 0.404\n",
      "27 16954.637 Train: 0.366 Val: 0.374 Test: 0.401 -- Best Val: 0.379 Test: 0.404\n",
      "28 16935.654 Train: 0.372 Val: 0.381 Test: 0.406 -- Best Val: 0.381 Test: 0.406\n",
      "29 16847.092 Train: 0.371 Val: 0.380 Test: 0.405 -- Best Val: 0.381 Test: 0.406\n",
      "30 16813.875 Train: 0.368 Val: 0.377 Test: 0.400 -- Best Val: 0.381 Test: 0.406\n",
      "31 16829.607 Train: 0.365 Val: 0.373 Test: 0.395 -- Best Val: 0.381 Test: 0.406\n",
      "32 16742.652 Train: 0.367 Val: 0.376 Test: 0.400 -- Best Val: 0.381 Test: 0.406\n",
      "33 16706.75 Train: 0.368 Val: 0.378 Test: 0.401 -- Best Val: 0.381 Test: 0.406\n",
      "34 16687.889 Train: 0.371 Val: 0.379 Test: 0.406 -- Best Val: 0.381 Test: 0.406\n"
     ]
    }
   ],
   "source": [
    "train_size = xdata.shape[0]\n",
    "batch_size = 10\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(np.hstack((xdata, ydata)))\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "# print(xdata.shape)\n",
    "num_epoch = 1000\n",
    "i = 0;\n",
    "best_val = 0\n",
    "test_val = 0\n",
    "\n",
    "while i < num_epoch:\n",
    "  loss = 0;\n",
    "  for batch in train_dataset:\n",
    "    xbatch = batch[:, :max_length_output]\n",
    "    ybatch = batch[:, max_length_output:]\n",
    "    loss += train_step(xbatch, ybatch)\n",
    "\n",
    "  ypred_test = cite(xtest, predict=True).numpy()\n",
    "  ypred_val = cite(xval, predict=True).numpy()\n",
    "  ypred_train = cite(xdata, predict=True).numpy()\n",
    "  print(i, loss.numpy(), perf(ypred_train, ypred_val, ypred_test, ydata, yval, ytest) )\n",
    "  i = i+1\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDwot8oPMYpT",
    "outputId": "8a535670-9356-4b82-a919-250f87832d00"
   },
   "outputs": [],
   "source": [
    "print_sentence_pred_label(xtest[1], ydata[1], ytest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "citation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
